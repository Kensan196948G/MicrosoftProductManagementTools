#!/usr/bin/env python3
"""
Microsoft 365 Management Tools - ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ 
ç½å®³å¾©æ—§è¨ˆç”»ãƒ»ãƒ‡ãƒ¼ã‚¿ä¿è­·ãƒ»è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§ãƒ†ã‚¹ãƒˆ
"""

import asyncio
import time
import json
import logging
import shutil
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path
import sqlite3
import threading
from dataclasses import dataclass, asdict
from enum import Enum
import subprocess
import hashlib
import zipfile
import tempfile
import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))


class BackupType(Enum):
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¿ã‚¤ãƒ—"""
    FULL = "full"
    INCREMENTAL = "incremental"
    DIFFERENTIAL = "differential"
    CONFIGURATION = "configuration"
    DATABASE = "database"


class BackupStatus(Enum):
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—çŠ¶æ…‹"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    EXPIRED = "expired"


class RecoveryType(Enum):
    """å¾©æ—§ã‚¿ã‚¤ãƒ—"""
    POINT_IN_TIME = "point_in_time"
    FULL_RESTORE = "full_restore"
    PARTIAL_RESTORE = "partial_restore"
    DISASTER_RECOVERY = "disaster_recovery"


@dataclass
class BackupJob:
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¸ãƒ§ãƒ–å®šç¾©"""
    id: str
    name: str
    backup_type: BackupType
    source_paths: List[str]
    destination_path: str
    schedule: str  # cronå½¢å¼
    retention_days: int
    compression: bool
    encryption: bool
    created_at: datetime
    last_run: Optional[datetime] = None
    next_run: Optional[datetime] = None
    status: BackupStatus = BackupStatus.PENDING


@dataclass
class BackupRecord:
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨˜éŒ²"""
    id: str
    job_id: str
    backup_type: BackupType
    status: BackupStatus
    start_time: datetime
    end_time: Optional[datetime]
    file_path: str
    file_size: int
    checksum: str
    compressed: bool
    encrypted: bool
    retention_until: datetime
    metadata: Dict[str, Any]


@dataclass
class RecoveryPlan:
    """å¾©æ—§è¨ˆç”»"""
    id: str
    name: str
    description: str
    recovery_type: RecoveryType
    priority: int
    rto_minutes: int  # Recovery Time Objective
    rpo_minutes: int  # Recovery Point Objective
    procedures: List[str]
    dependencies: List[str]
    contacts: List[str]
    last_tested: Optional[datetime] = None


class BackupRecoverySystem:
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config_path: str = None):
        self.config = self._load_config(config_path)
        self.backup_db = self._init_backup_database()
        self.active_jobs: Dict[str, BackupJob] = {}
        self.backup_scheduler_active = False
        self.recovery_plans: Dict[str, RecoveryPlan] = {}
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.backup_root = Path(self.config["backup"]["root_directory"])
        self.backup_root.mkdir(parents=True, exist_ok=True)
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('Tests/production_enterprise/logs/backup_recovery.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        default_config = {
            "backup": {
                "root_directory": "Tests/production_enterprise/backups",
                "max_parallel_jobs": 3,
                "compression_level": 6,
                "encryption_enabled": True,
                "checksum_algorithm": "sha256",
                "default_retention_days": 30
            },
            "recovery": {
                "staging_directory": "Tests/production_enterprise/recovery_staging",
                "verification_required": True,
                "max_recovery_time_minutes": 240,  # 4æ™‚é–“
                "notification_enabled": True
            },
            "disaster_recovery": {
                "backup_sites": ["site1", "site2"],
                "replication_enabled": True,
                "failover_threshold_minutes": 30,
                "auto_failover_enabled": False
            },
            "monitoring": {
                "health_check_interval_minutes": 30,
                "backup_verification_enabled": True,
                "retention_cleanup_interval_hours": 24
            }
        }
        
        if config_path and Path(config_path).exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                default_config.update(user_config)
        
        return default_config
    
    def _init_backup_database(self) -> sqlite3.Connection:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        db_path = Path("Tests/production_enterprise/backup_recovery.db")
        db_path.parent.mkdir(parents=True, exist_ok=True)
        
        conn = sqlite3.connect(str(db_path), check_same_thread=False)
        cursor = conn.cursor()
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¸ãƒ§ãƒ–ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS backup_jobs (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                backup_type TEXT NOT NULL,
                source_paths TEXT NOT NULL,
                destination_path TEXT NOT NULL,
                schedule TEXT NOT NULL,
                retention_days INTEGER NOT NULL,
                compression BOOLEAN NOT NULL,
                encryption BOOLEAN NOT NULL,
                created_at TEXT NOT NULL,
                last_run TEXT,
                next_run TEXT,
                status TEXT NOT NULL
            )
        ''')
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨˜éŒ²ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS backup_records (
                id TEXT PRIMARY KEY,
                job_id TEXT NOT NULL,
                backup_type TEXT NOT NULL,
                status TEXT NOT NULL,
                start_time TEXT NOT NULL,
                end_time TEXT,
                file_path TEXT NOT NULL,
                file_size INTEGER NOT NULL,
                checksum TEXT NOT NULL,
                compressed BOOLEAN NOT NULL,
                encrypted BOOLEAN NOT NULL,
                retention_until TEXT NOT NULL,
                metadata TEXT
            )
        ''')
        
        # å¾©æ—§è¨ˆç”»ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS recovery_plans (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                description TEXT NOT NULL,
                recovery_type TEXT NOT NULL,
                priority INTEGER NOT NULL,
                rto_minutes INTEGER NOT NULL,
                rpo_minutes INTEGER NOT NULL,
                procedures TEXT NOT NULL,
                dependencies TEXT NOT NULL,
                contacts TEXT NOT NULL,
                last_tested TEXT
            )
        ''')
        
        conn.commit()
        return conn
    
    async def start_backup_system(self):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹"""
        self.backup_scheduler_active = True
        self.logger.info("ğŸ—„ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹ - ç½å®³å¾©æ—§ä½“åˆ¶æ§‹ç¯‰")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¸ãƒ§ãƒ–ã®ä½œæˆ
        await self._create_default_backup_jobs()
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¾©æ—§è¨ˆç”»ã®ä½œæˆ
        await self._create_default_recovery_plans()
        
        # ä¸¦åˆ—ã‚·ã‚¹ãƒ†ãƒ ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
        system_tasks = [
            self._backup_scheduler(),
            self._backup_monitor(),
            self._retention_manager(),
            self._recovery_plan_tester()
        ]
        
        await asyncio.gather(*system_tasks)
    
    async def stop_backup_system(self):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ åœæ­¢"""
        self.backup_scheduler_active = False
        self.logger.info("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ åœæ­¢")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¯ãƒ­ãƒ¼ã‚º
        self.backup_db.close()
    
    async def _create_default_backup_jobs(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¸ãƒ§ãƒ–ä½œæˆ"""
        default_jobs = [
            {
                "name": "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ—¥æ¬¡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—",
                "backup_type": BackupType.CONFIGURATION,
                "source_paths": ["Config/", "Scripts/Common/"],
                "schedule": "0 2 * * *",  # æ¯æ—¥åˆå‰2æ™‚
                "retention_days": 30
            },
            {
                "name": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ—¥æ¬¡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—",
                "backup_type": BackupType.DATABASE,
                "source_paths": ["Tests/production_enterprise/*.db"],
                "schedule": "0 3 * * *",  # æ¯æ—¥åˆå‰3æ™‚
                "retention_days": 7
            },
            {
                "name": "ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«é€±æ¬¡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—",
                "backup_type": BackupType.FULL,
                "source_paths": ["Tests/production_enterprise/logs/"],
                "schedule": "0 4 * * 0",  # æ¯é€±æ—¥æ›œåˆå‰4æ™‚
                "retention_days": 90
            },
            {
                "name": "ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—",
                "backup_type": BackupType.FULL,
                "source_paths": ["src/", "Apps/", "Scripts/"],
                "schedule": "0 1 1 * *",  # æ¯æœˆ1æ—¥åˆå‰1æ™‚
                "retention_days": 365
            }
        ]
        
        for job_config in default_jobs:
            await self._create_backup_job(**job_config)
    
    async def _create_default_recovery_plans(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¾©æ—§è¨ˆç”»ä½œæˆ"""
        default_plans = [
            {
                "name": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹éšœå®³å¾©æ—§",
                "description": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å®Œå…¨éšœå®³ã‹ã‚‰ã®å¾©æ—§æ‰‹é †",
                "recovery_type": RecoveryType.FULL_RESTORE,
                "priority": 1,
                "rto_minutes": 60,
                "rpo_minutes": 15,
                "procedures": [
                    "1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢",
                    "2. æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ç‰¹å®š",
                    "3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å¾©å…ƒ",
                    "4. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å¾©å…ƒï¼ˆå¯èƒ½ãªå ´åˆï¼‰",
                    "5. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹",
                    "6. æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ",
                    "7. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ¥ç¶šãƒ†ã‚¹ãƒˆ"
                ],
                "dependencies": ["ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«", "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µãƒ¼ãƒãƒ¼"],
                "contacts": ["dba@company.com", "ops@company.com"]
            },
            {
                "name": "ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³éšœå®³å¾©æ—§",
                "description": "ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚µãƒ¼ãƒãƒ¼éšœå®³ã‹ã‚‰ã®å¾©æ—§",
                "recovery_type": RecoveryType.DISASTER_RECOVERY,
                "priority": 2,
                "rto_minutes": 120,
                "rpo_minutes": 30,
                "procedures": [
                    "1. ä»£æ›¿ã‚µãƒ¼ãƒãƒ¼ã®æº–å‚™",
                    "2. æœ€æ–°ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒ",
                    "3. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¾©å…ƒ",
                    "4. ä¾å­˜é–¢ä¿‚ã®ç¢ºèª",
                    "5. ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹",
                    "6. æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ",
                    "7. ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼è¨­å®šæ›´æ–°"
                ],
                "dependencies": ["ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—", "è¨­å®šãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"],
                "contacts": ["devops@company.com", "app-team@company.com"]
            },
            {
                "name": "å®Œå…¨ã‚·ã‚¹ãƒ†ãƒ å¾©æ—§",
                "description": "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼å®Œå…¨éšœå®³ã‹ã‚‰ã®å¾©æ—§",
                "recovery_type": RecoveryType.DISASTER_RECOVERY,
                "priority": 3,
                "rto_minutes": 240,
                "rpo_minutes": 60,
                "procedures": [
                    "1. ç½å®³å¾©æ—§ã‚µã‚¤ãƒˆã®æœ‰åŠ¹åŒ–",
                    "2. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š",
                    "3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©æ—§",
                    "4. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å¾©æ—§",
                    "5. ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ å¾©æ—§",
                    "6. DNSåˆ‡ã‚Šæ›¿ãˆ",
                    "7. å…¨ã‚·ã‚¹ãƒ†ãƒ æ©Ÿèƒ½ç¢ºèª"
                ],
                "dependencies": ["ç½å®³å¾©æ—§ã‚µã‚¤ãƒˆ", "å…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"],
                "contacts": ["incident-commander@company.com", "all-hands@company.com"]
            }
        ]
        
        for plan_config in default_plans:
            await self._create_recovery_plan(**plan_config)
    
    async def _create_backup_job(self, name: str, backup_type: BackupType, source_paths: List[str], 
                                schedule: str, retention_days: int) -> str:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¸ãƒ§ãƒ–ä½œæˆ"""
        job_id = f"backup_{int(time.time())}"
        
        job = BackupJob(
            id=job_id,
            name=name,
            backup_type=backup_type,
            source_paths=source_paths,
            destination_path=str(self.backup_root / backup_type.value),
            schedule=schedule,
            retention_days=retention_days,
            compression=self.config["backup"]["compression_level"] > 0,
            encryption=self.config["backup"]["encryption_enabled"],
            created_at=datetime.now(),
            next_run=self._calculate_next_run(schedule)
        )
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¸ãƒ§ãƒ–ã«è¿½åŠ 
        self.active_jobs[job_id] = job
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        await self._store_backup_job(job)
        
        self.logger.info(f"ğŸ“‹ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¸ãƒ§ãƒ–ä½œæˆ: {name} - {job_id}")
        
        return job_id
    
    async def _create_recovery_plan(self, name: str, description: str, recovery_type: RecoveryType,
                                  priority: int, rto_minutes: int, rpo_minutes: int,
                                  procedures: List[str], dependencies: List[str], contacts: List[str]) -> str:
        """å¾©æ—§è¨ˆç”»ä½œæˆ"""
        plan_id = f"recovery_plan_{int(time.time())}"
        
        plan = RecoveryPlan(
            id=plan_id,
            name=name,
            description=description,
            recovery_type=recovery_type,
            priority=priority,
            rto_minutes=rto_minutes,
            rpo_minutes=rpo_minutes,
            procedures=procedures,
            dependencies=dependencies,
            contacts=contacts
        )
        
        # å¾©æ—§è¨ˆç”»ã«è¿½åŠ 
        self.recovery_plans[plan_id] = plan
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        await self._store_recovery_plan(plan)
        
        self.logger.info(f"ğŸ“‹ å¾©æ—§è¨ˆç”»ä½œæˆ: {name} - {plan_id}")
        
        return plan_id
    
    def _calculate_next_run(self, schedule: str) -> datetime:
        """æ¬¡å›å®Ÿè¡Œæ™‚åˆ»è¨ˆç®—ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ croniterã‚„APSchedulerã‚’ä½¿ç”¨
        # ã“ã“ã§ã¯ç°¡ç•¥çš„ã«1æ™‚é–“å¾Œã¨ã™ã‚‹
        return datetime.now() + timedelta(hours=1)
    
    async def _backup_scheduler(self):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼"""
        while self.backup_scheduler_active:
            try:
                current_time = datetime.now()
                
                for job_id, job in list(self.active_jobs.items()):
                    if job.next_run and current_time >= job.next_run:
                        if job.status != BackupStatus.IN_PROGRESS:
                            await self._execute_backup_job(job)
                
                await asyncio.sleep(60)  # 1åˆ†é–“éš”
            
            except Exception as e:
                self.logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
                await asyncio.sleep(30)
    
    async def _execute_backup_job(self, job: BackupJob) -> bool:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¸ãƒ§ãƒ–å®Ÿè¡Œ"""
        try:
            self.logger.info(f"ğŸ—„ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹: {job.name}")
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
            job.status = BackupStatus.IN_PROGRESS
            job.last_run = datetime.now()
            await self._update_backup_job_in_db(job)
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
            backup_record = await self._perform_backup(job)
            
            if backup_record:
                job.status = BackupStatus.COMPLETED
                self.logger.info(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {job.name} - {backup_record.file_size} bytes")
            else:
                job.status = BackupStatus.FAILED
                self.logger.error(f"âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—: {job.name}")
            
            # æ¬¡å›å®Ÿè¡Œæ™‚åˆ»æ›´æ–°
            job.next_run = self._calculate_next_run(job.schedule)
            await self._update_backup_job_in_db(job)
            
            return backup_record is not None
        
        except Exception as e:
            job.status = BackupStatus.FAILED
            await self._update_backup_job_in_db(job)
            self.logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _perform_backup(self, job: BackupJob) -> Optional[BackupRecord]:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œ"""
        try:
            start_time = datetime.now()
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
            timestamp = start_time.strftime("%Y%m%d_%H%M%S")
            backup_filename = f"{job.backup_type.value}_{timestamp}.zip"
            backup_path = Path(job.destination_path) / backup_filename
            backup_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åé›†
            source_files = []
            for source_pattern in job.source_paths:
                source_path = Path(source_pattern)
                if source_path.is_dir():
                    source_files.extend(source_path.rglob("*"))
                elif source_path.exists():
                    source_files.append(source_path)
                else:
                    # ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰å¯¾å¿œ
                    source_files.extend(Path(".").glob(source_pattern))
            
            # ZIPåœ§ç¸®ã§ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            with zipfile.ZipFile(backup_path, 'w', 
                               compression=zipfile.ZIP_DEFLATED if job.compression else zipfile.ZIP_STORED) as zipf:
                for file_path in source_files:
                    if file_path.is_file():
                        try:
                            zipf.write(file_path, file_path.relative_to(Path(".")))
                        except Exception as e:
                            self.logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ ã‚¹ã‚­ãƒƒãƒ—: {file_path} - {e}")
            
            end_time = datetime.now()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ãƒã‚§ãƒƒã‚¯ã‚µãƒ è¨ˆç®—
            file_size = backup_path.stat().st_size
            checksum = self._calculate_checksum(backup_path)
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨˜éŒ²ä½œæˆ
            record_id = f"backup_record_{int(time.time())}"
            backup_record = BackupRecord(
                id=record_id,
                job_id=job.id,
                backup_type=job.backup_type,
                status=BackupStatus.COMPLETED,
                start_time=start_time,
                end_time=end_time,
                file_path=str(backup_path),
                file_size=file_size,
                checksum=checksum,
                compressed=job.compression,
                encrypted=job.encryption,
                retention_until=datetime.now() + timedelta(days=job.retention_days),
                metadata={
                    "source_file_count": len(source_files),
                    "duration_seconds": (end_time - start_time).total_seconds()
                }
            )
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜éŒ²ä¿å­˜
            await self._store_backup_record(backup_record)
            
            return backup_record
        
        except Exception as e:
            self.logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _calculate_checksum(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯ã‚µãƒ è¨ˆç®—"""
        hash_algorithm = self.config["backup"]["checksum_algorithm"]
        hasher = hashlib.new(hash_algorithm)
        
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        
        return hasher.hexdigest()
    
    async def _backup_monitor(self):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç›£è¦–"""
        while self.backup_scheduler_active:
            try:
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—ã®ç›£è¦–
                failed_jobs = [job for job in self.active_jobs.values() if job.status == BackupStatus.FAILED]
                
                for job in failed_jobs:
                    self.logger.warning(f"ğŸš¨ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—æ¤œå‡º: {job.name}")
                    # ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã‚„ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡ã‚’å®Ÿè£…
                
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                if self.config["monitoring"]["backup_verification_enabled"]:
                    await self._verify_recent_backups()
                
                await asyncio.sleep(self.config["monitoring"]["health_check_interval_minutes"] * 60)
            
            except Exception as e:
                self.logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                await asyncio.sleep(300)
    
    async def _verify_recent_backups(self):
        """æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®æ¤œè¨¼"""
        try:
            # éå»24æ™‚é–“ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’æ¤œè¨¼
            cursor = self.backup_db.cursor()
            cursor.execute('''
                SELECT * FROM backup_records 
                WHERE start_time > datetime('now', '-1 day')
                AND status = 'completed'
            ''')
            
            recent_backups = cursor.fetchall()
            
            for backup_row in recent_backups:
                file_path = Path(backup_row[6])  # file_pathåˆ—
                stored_checksum = backup_row[8]  # checksumåˆ—
                
                if file_path.exists():
                    current_checksum = self._calculate_checksum(file_path)
                    if current_checksum != stored_checksum:
                        self.logger.error(f"ğŸš¨ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ•´åˆæ€§ã‚¨ãƒ©ãƒ¼: {file_path}")
                else:
                    self.logger.error(f"ğŸš¨ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ä¸åœ¨: {file_path}")
        
        except Exception as e:
            self.logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def _retention_manager(self):
        """ä¿å­˜æœŸé–“ç®¡ç†"""
        while self.backup_scheduler_active:
            try:
                current_time = datetime.now()
                
                # ä¿å­˜æœŸé–“åˆ‡ã‚Œã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤
                cursor = self.backup_db.cursor()
                cursor.execute('''
                    SELECT * FROM backup_records 
                    WHERE retention_until < ?
                    AND status = 'completed'
                ''', (current_time.isoformat(),))
                
                expired_backups = cursor.fetchall()
                
                for backup_row in expired_backups:
                    record_id = backup_row[0]
                    file_path = Path(backup_row[6])
                    
                    try:
                        if file_path.exists():
                            file_path.unlink()
                            self.logger.info(f"ğŸ—‘ï¸ æœŸé™åˆ‡ã‚Œãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤: {file_path}")
                        
                        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æœŸé™åˆ‡ã‚Œã«æ›´æ–°
                        cursor.execute('''
                            UPDATE backup_records 
                            SET status = 'expired' 
                            WHERE id = ?
                        ''', (record_id,))
                        
                    except Exception as e:
                        self.logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
                
                self.backup_db.commit()
                
                await asyncio.sleep(self.config["monitoring"]["retention_cleanup_interval_hours"] * 3600)
            
            except Exception as e:
                self.logger.error(f"ä¿å­˜æœŸé–“ç®¡ç†ã‚¨ãƒ©ãƒ¼: {e}")
                await asyncio.sleep(3600)
    
    async def _recovery_plan_tester(self):
        """å¾©æ—§è¨ˆç”»ãƒ†ã‚¹ã‚¿ãƒ¼"""
        while self.backup_scheduler_active:
            try:
                # æœˆæ¬¡ã§å¾©æ—§è¨ˆç”»ã‚’ãƒ†ã‚¹ãƒˆ
                for plan_id, plan in self.recovery_plans.items():
                    if plan.last_tested is None or \
                       (datetime.now() - plan.last_tested).days >= 30:
                        
                        await self._test_recovery_plan(plan)
                
                await asyncio.sleep(24 * 3600)  # æ—¥æ¬¡ãƒã‚§ãƒƒã‚¯
            
            except Exception as e:
                self.logger.error(f"å¾©æ—§è¨ˆç”»ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                await asyncio.sleep(3600)
    
    async def _test_recovery_plan(self, plan: RecoveryPlan) -> bool:
        """å¾©æ—§è¨ˆç”»ãƒ†ã‚¹ãƒˆ"""
        try:
            self.logger.info(f"ğŸ§ª å¾©æ—§è¨ˆç”»ãƒ†ã‚¹ãƒˆé–‹å§‹: {plan.name}")
            
            # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã®å¾©æ—§æ‰‹é †å®Ÿè¡Œï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
            test_results = {
                "plan_id": plan.id,
                "test_start": datetime.now(),
                "procedures_tested": len(plan.procedures),
                "success": True,
                "issues": []
            }
            
            # å„æ‰‹é †ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            for i, procedure in enumerate(plan.procedures):
                self.logger.info(f"æ‰‹é † {i+1}: {procedure}")
                await asyncio.sleep(1)  # å‡¦ç†æ™‚é–“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
                
                # ãƒ©ãƒ³ãƒ€ãƒ ã«å¤±æ•—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆ10%ã®ç¢ºç‡ï¼‰
                import random
                if random.random() < 0.1:
                    test_results["issues"].append(f"æ‰‹é † {i+1} ã§å•é¡Œç™ºç”Ÿ: {procedure}")
            
            test_results["test_end"] = datetime.now()
            test_results["duration_minutes"] = (test_results["test_end"] - test_results["test_start"]).total_seconds() / 60
            
            # RTO/RPO ãƒã‚§ãƒƒã‚¯
            if test_results["duration_minutes"] > plan.rto_minutes:
                test_results["issues"].append(f"RTOè¶…é: {test_results['duration_minutes']:.1f}åˆ† > {plan.rto_minutes}åˆ†")
                test_results["success"] = False
            
            # ãƒ†ã‚¹ãƒˆçµæœè¨˜éŒ²
            plan.last_tested = datetime.now()
            await self._update_recovery_plan_in_db(plan)
            
            # ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            await self._save_recovery_test_report(test_results)
            
            if test_results["success"]:
                self.logger.info(f"âœ… å¾©æ—§è¨ˆç”»ãƒ†ã‚¹ãƒˆæˆåŠŸ: {plan.name}")
            else:
                self.logger.warning(f"âš ï¸ å¾©æ—§è¨ˆç”»ãƒ†ã‚¹ãƒˆã§å•é¡Œç™ºè¦‹: {plan.name} - {len(test_results['issues'])}ä»¶")
            
            return test_results["success"]
        
        except Exception as e:
            self.logger.error(f"å¾©æ—§è¨ˆç”»ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def execute_recovery(self, plan_id: str, recovery_point: datetime = None) -> bool:
        """å¾©æ—§å®Ÿè¡Œ"""
        if plan_id not in self.recovery_plans:
            raise ValueError(f"å¾©æ—§è¨ˆç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {plan_id}")
        
        plan = self.recovery_plans[plan_id]
        
        try:
            self.logger.critical(f"ğŸš¨ å¾©æ—§å®Ÿè¡Œé–‹å§‹: {plan.name}")
            
            # å¾©æ—§å®Ÿè¡Œè¨˜éŒ²
            recovery_record = {
                "plan_id": plan_id,
                "recovery_start": datetime.now(),
                "recovery_point": recovery_point or datetime.now(),
                "status": "in_progress",
                "procedures_completed": 0,
                "total_procedures": len(plan.procedures)
            }
            
            # å¾©æ—§æ‰‹é †å®Ÿè¡Œ
            for i, procedure in enumerate(plan.procedures):
                self.logger.info(f"å¾©æ—§æ‰‹é † {i+1}: {procedure}")
                
                # å®Ÿéš›ã®å¾©æ—§å‡¦ç†ã‚’ã“ã“ã«å®Ÿè£…
                success = await self._execute_recovery_procedure(procedure, recovery_point)
                
                if success:
                    recovery_record["procedures_completed"] += 1
                else:
                    recovery_record["status"] = "failed"
                    recovery_record["failed_procedure"] = procedure
                    break
            
            recovery_record["recovery_end"] = datetime.now()
            recovery_record["duration_minutes"] = (recovery_record["recovery_end"] - recovery_record["recovery_start"]).total_seconds() / 60
            
            if recovery_record["procedures_completed"] == recovery_record["total_procedures"]:
                recovery_record["status"] = "completed"
                self.logger.info(f"âœ… å¾©æ—§å®Œäº†: {plan.name} - {recovery_record['duration_minutes']:.1f}åˆ†")
            else:
                self.logger.error(f"âŒ å¾©æ—§å¤±æ•—: {plan.name}")
            
            # å¾©æ—§è¨˜éŒ²ä¿å­˜
            await self._save_recovery_record(recovery_record)
            
            return recovery_record["status"] == "completed"
        
        except Exception as e:
            self.logger.error(f"å¾©æ—§å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _execute_recovery_procedure(self, procedure: str, recovery_point: datetime) -> bool:
        """å¾©æ—§æ‰‹é †å®Ÿè¡Œ"""
        try:
            # æ‰‹é †ã«å¿œã˜ãŸå¾©æ—§å‡¦ç†
            if "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹" in procedure:
                return await self._restore_database(recovery_point)
            elif "ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³" in procedure:
                return await self._restore_application(recovery_point)
            elif "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«" in procedure:
                return await self._restore_configuration(recovery_point)
            else:
                # ãã®ä»–ã®æ‰‹é †
                await asyncio.sleep(2)  # å‡¦ç†æ™‚é–“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
                return True
        
        except Exception as e:
            self.logger.error(f"å¾©æ—§æ‰‹é †å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _restore_database(self, recovery_point: datetime) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©æ—§"""
        try:
            # æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ç‰¹å®š
            cursor = self.backup_db.cursor()
            cursor.execute('''
                SELECT * FROM backup_records 
                WHERE backup_type = 'database'
                AND status = 'completed'
                AND start_time <= ?
                ORDER BY start_time DESC
                LIMIT 1
            ''', (recovery_point.isoformat(),))
            
            backup_record = cursor.fetchone()
            
            if backup_record:
                backup_path = Path(backup_record[6])
                if backup_path.exists():
                    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®å¾©å…ƒï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
                    self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©å…ƒ: {backup_path}")
                    await asyncio.sleep(3)
                    return True
            
            return False
        
        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©æ—§ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _restore_application(self, recovery_point: datetime) -> bool:
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å¾©æ—§"""
        try:
            # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å¾©å…ƒï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
            self.logger.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å¾©å…ƒå®Ÿè¡Œ")
            await asyncio.sleep(5)
            return True
        
        except Exception as e:
            self.logger.error(f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å¾©æ—§ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _restore_configuration(self, recovery_point: datetime) -> bool:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¾©æ—§"""
        try:
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å¾©å…ƒï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
            self.logger.info("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¾©å…ƒå®Ÿè¡Œ")
            await asyncio.sleep(1)
            return True
        
        except Exception as e:
            self.logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¾©æ—§ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _store_backup_job(self, job: BackupJob):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¸ãƒ§ãƒ–ä¿å­˜"""
        try:
            cursor = self.backup_db.cursor()
            cursor.execute('''
                INSERT INTO backup_jobs 
                (id, name, backup_type, source_paths, destination_path, schedule, 
                 retention_days, compression, encryption, created_at, last_run, next_run, status)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                job.id, job.name, job.backup_type.value, json.dumps(job.source_paths),
                job.destination_path, job.schedule, job.retention_days, job.compression,
                job.encryption, job.created_at.isoformat(),
                job.last_run.isoformat() if job.last_run else None,
                job.next_run.isoformat() if job.next_run else None,
                job.status.value
            ))
            self.backup_db.commit()
        
        except Exception as e:
            self.logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¸ãƒ§ãƒ–ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def _update_backup_job_in_db(self, job: BackupJob):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¸ãƒ§ãƒ–DBæ›´æ–°"""
        try:
            cursor = self.backup_db.cursor()
            cursor.execute('''
                UPDATE backup_jobs 
                SET last_run = ?, next_run = ?, status = ?
                WHERE id = ?
            ''', (
                job.last_run.isoformat() if job.last_run else None,
                job.next_run.isoformat() if job.next_run else None,
                job.status.value,
                job.id
            ))
            self.backup_db.commit()
        
        except Exception as e:
            self.logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¸ãƒ§ãƒ–æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def _store_backup_record(self, record: BackupRecord):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨˜éŒ²ä¿å­˜"""
        try:
            cursor = self.backup_db.cursor()
            cursor.execute('''
                INSERT INTO backup_records 
                (id, job_id, backup_type, status, start_time, end_time, file_path, 
                 file_size, checksum, compressed, encrypted, retention_until, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                record.id, record.job_id, record.backup_type.value, record.status.value,
                record.start_time.isoformat(),
                record.end_time.isoformat() if record.end_time else None,
                record.file_path, record.file_size, record.checksum, record.compressed,
                record.encrypted, record.retention_until.isoformat(),
                json.dumps(record.metadata)
            ))
            self.backup_db.commit()
        
        except Exception as e:
            self.logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨˜éŒ²ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def _store_recovery_plan(self, plan: RecoveryPlan):
        """å¾©æ—§è¨ˆç”»ä¿å­˜"""
        try:
            cursor = self.backup_db.cursor()
            cursor.execute('''
                INSERT INTO recovery_plans 
                (id, name, description, recovery_type, priority, rto_minutes, rpo_minutes, 
                 procedures, dependencies, contacts, last_tested)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                plan.id, plan.name, plan.description, plan.recovery_type.value,
                plan.priority, plan.rto_minutes, plan.rpo_minutes,
                json.dumps(plan.procedures), json.dumps(plan.dependencies),
                json.dumps(plan.contacts),
                plan.last_tested.isoformat() if plan.last_tested else None
            ))
            self.backup_db.commit()
        
        except Exception as e:
            self.logger.error(f"å¾©æ—§è¨ˆç”»ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def _update_recovery_plan_in_db(self, plan: RecoveryPlan):
        """å¾©æ—§è¨ˆç”»DBæ›´æ–°"""
        try:
            cursor = self.backup_db.cursor()
            cursor.execute('''
                UPDATE recovery_plans 
                SET last_tested = ?
                WHERE id = ?
            ''', (
                plan.last_tested.isoformat() if plan.last_tested else None,
                plan.id
            ))
            self.backup_db.commit()
        
        except Exception as e:
            self.logger.error(f"å¾©æ—§è¨ˆç”»æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def _save_recovery_test_report(self, test_results: Dict[str, Any]):
        """å¾©æ—§ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        try:
            report_file = Path(f"Tests/production_enterprise/recovery_test_reports/recovery_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            report_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(test_results, f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info(f"ğŸ“Š å¾©æ—§ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
        
        except Exception as e:
            self.logger.error(f"å¾©æ—§ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def _save_recovery_record(self, recovery_record: Dict[str, Any]):
        """å¾©æ—§è¨˜éŒ²ä¿å­˜"""
        try:
            record_file = Path(f"Tests/production_enterprise/recovery_records/recovery_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            record_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(record_file, 'w', encoding='utf-8') as f:
                json.dump(recovery_record, f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info(f"ğŸ“Š å¾©æ—§è¨˜éŒ²ä¿å­˜: {record_file}")
        
        except Exception as e:
            self.logger.error(f"å¾©æ—§è¨˜éŒ²ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_backup_status(self) -> Dict[str, Any]:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—çŠ¶æ…‹å–å¾—"""
        return {
            "timestamp": datetime.now().isoformat(),
            "active_jobs": len(self.active_jobs),
            "recovery_plans": len(self.recovery_plans),
            "backup_scheduler_active": self.backup_scheduler_active,
            "recent_backup_count": self._get_recent_backup_count(),
            "total_backup_size": self._get_total_backup_size()
        }
    
    def _get_recent_backup_count(self) -> int:
        """æœ€è¿‘ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ•°å–å¾—"""
        try:
            cursor = self.backup_db.cursor()
            cursor.execute('''
                SELECT COUNT(*) FROM backup_records 
                WHERE start_time > datetime('now', '-7 days')
                AND status = 'completed'
            ''')
            return cursor.fetchone()[0]
        except:
            return 0
    
    def _get_total_backup_size(self) -> int:
        """ç·ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚µã‚¤ã‚ºå–å¾—"""
        try:
            cursor = self.backup_db.cursor()
            cursor.execute('''
                SELECT SUM(file_size) FROM backup_records 
                WHERE status = 'completed'
            ''')
            result = cursor.fetchone()[0]
            return result if result else 0
        except:
            return 0


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    backup_system = BackupRecoverySystem()
    
    try:
        await backup_system.start_backup_system()
    except KeyboardInterrupt:
        print("\nãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ ã‚’åœæ­¢ä¸­...")
        await backup_system.stop_backup_system()
    except Exception as e:
        logging.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        await backup_system.stop_backup_system()


if __name__ == "__main__":
    asyncio.run(main())