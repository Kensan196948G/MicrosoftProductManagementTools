#!/usr/bin/env python3
"""
Comprehensive Test Coverage Analyzer - 90% Target Achievement
QA Engineer (dev2) - Test Coverage & Quality Metrics Specialist

ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸90%ä»¥ä¸Šé”æˆãƒ»å“è³ªæŒ‡æ¨™æ¸¬å®šã‚·ã‚¹ãƒ†ãƒ ï¼š
- 1,037ãƒ†ã‚¹ãƒˆé–¢æ•°ã®åŒ…æ‹¬çš„ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®š
- Python/PowerShell/React/TypeScriptã™ã¹ã¦ã®ã‚«ãƒãƒ¬ãƒƒã‚¸çµ±åˆ
- 26æ©Ÿèƒ½å®Œå…¨ã‚«ãƒãƒ¬ãƒƒã‚¸æ¤œè¨¼
- å“è³ªæŒ‡æ¨™ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†åˆ†æ
- ã‚«ãƒãƒ¬ãƒƒã‚¸æ”¹å–„ææ¡ˆè‡ªå‹•ç”Ÿæˆ
"""
import os
import sys
import json
import subprocess
import logging
import xml.etree.ElementTree as ET
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import pytest
import coverage
import ast

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
PROJECT_ROOT = Path(__file__).parent.parent.parent.absolute()
sys.path.insert(0, str(PROJECT_ROOT))

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ComprehensiveCoverageAnalyzer:
    """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.project_root = PROJECT_ROOT
        self.tests_dir = self.project_root / "Tests"
        self.src_dir = self.project_root / "src"
        self.frontend_dir = self.project_root / "frontend"
        self.apps_dir = self.project_root / "Apps"
        self.scripts_dir = self.project_root / "Scripts"
        
        self.coverage_dir = self.tests_dir / "coverage_measurement"
        self.coverage_dir.mkdir(exist_ok=True)
        
        self.reports_dir = self.coverage_dir / "reports"
        self.reports_dir.mkdir(exist_ok=True)
        
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™è¨­å®š
        self.coverage_targets = {
            "overall": 90.0,
            "python": 90.0,
            "react_typescript": 85.0,
            "powershell": 75.0,  # PowerShellã¯å°‘ã—ä½ã‚
            "api_endpoints": 95.0,
            "26_features": 100.0  # 26æ©Ÿèƒ½ã¯å®Œå…¨ã‚«ãƒãƒ¬ãƒƒã‚¸
        }
        
        # å“è³ªæŒ‡æ¨™é–¾å€¤
        self.quality_thresholds = {
            "complexity_score": 8.0,
            "maintainability_index": 70.0,
            "test_failure_rate": 5.0,
            "performance_score": 80.0
        }
        
    def discover_all_source_files(self) -> Dict[str, List[str]]:
        """å…¨ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹"""
        logger.info("ğŸ” Discovering all source files...")
        
        source_files = {
            "python": [],
            "powershell": [],
            "typescript": [],
            "javascript": [],
            "test_files": []
        }
        
        # Pythonãƒ•ã‚¡ã‚¤ãƒ«
        for pattern in ["**/*.py"]:
            for py_file in self.project_root.glob(pattern):
                if any(exclude in str(py_file) for exclude in [".git", "__pycache__", "node_modules", "venv", ".venv"]):
                    continue
                
                if "test_" in py_file.name or py_file.parent.name in ["tests", "Tests"]:
                    source_files["test_files"].append(str(py_file))
                else:
                    source_files["python"].append(str(py_file))
        
        # PowerShellãƒ•ã‚¡ã‚¤ãƒ«
        for pattern in ["**/*.ps1", "**/*.psm1"]:
            for ps_file in self.project_root.glob(pattern):
                if ".git" not in str(ps_file):
                    source_files["powershell"].append(str(ps_file))
        
        # TypeScript/JavaScriptãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ï¼‰
        if self.frontend_dir.exists():
            for pattern in ["**/*.ts", "**/*.tsx"]:
                for ts_file in self.frontend_dir.glob(pattern):
                    if any(exclude in str(ts_file) for exclude in ["node_modules", "dist", "build"]):
                        continue
                    source_files["typescript"].append(str(ts_file))
            
            for pattern in ["**/*.js", "**/*.jsx"]:
                for js_file in self.frontend_dir.glob(pattern):
                    if any(exclude in str(js_file) for exclude in ["node_modules", "dist", "build"]):
                        continue
                    source_files["javascript"].append(str(js_file))
        
        # çµ±è¨ˆ
        total_files = sum(len(files) for files in source_files.values())
        logger.info(f"Found {total_files} source files:")
        for file_type, files in source_files.items():
            logger.info(f"  {file_type}: {len(files)} files")
        
        return source_files
    
    def analyze_python_coverage(self) -> Dict[str, Any]:
        """Python ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ"""
        logger.info("ğŸ Analyzing Python test coverage...")
        
        # coverage.pyå®Ÿè¡Œ
        coverage_data = {
            "execution_status": "unknown",
            "coverage_percentage": 0.0,
            "lines_covered": 0,
            "lines_total": 0,
            "files_analyzed": 0,
            "missing_lines": [],
            "detailed_results": {}
        }
        
        try:
            # pytest with coverageå®Ÿè¡Œ
            cmd = [
                "python", "-m", "pytest",
                str(self.tests_dir),
                "--cov=src",
                "--cov=Tests", 
                "--cov=Apps",
                "--cov=Scripts",
                f"--cov-report=html:{self.reports_dir}/python_coverage_html",
                f"--cov-report=xml:{self.reports_dir}/python_coverage.xml",
                f"--cov-report=json:{self.reports_dir}/python_coverage.json",
                "--cov-report=term-missing",
                "--cov-fail-under=90",
                "-v", "--tb=short",
                "--maxfail=5"
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
            
            coverage_data["execution_status"] = "completed"
            coverage_data["exit_code"] = result.returncode
            coverage_data["stdout_lines"] = len(result.stdout.splitlines())
            coverage_data["stderr_lines"] = len(result.stderr.splitlines())
            
            # JSON coverageçµæœèª­ã¿è¾¼ã¿
            json_coverage_file = self.reports_dir / "python_coverage.json"
            if json_coverage_file.exists():
                with open(json_coverage_file) as f:
                    coverage_json = json.load(f)
                    
                    totals = coverage_json.get("totals", {})
                    coverage_data["coverage_percentage"] = totals.get("percent_covered", 0.0)
                    coverage_data["lines_covered"] = totals.get("covered_lines", 0)
                    coverage_data["lines_total"] = totals.get("num_statements", 0)
                    coverage_data["files_analyzed"] = len(coverage_json.get("files", {}))
                    
                    # è©³ç´°çµæœ
                    coverage_data["detailed_results"] = coverage_json.get("files", {})
            
            # XML coverageçµæœèª­ã¿è¾¼ã¿ï¼ˆè¿½åŠ åˆ†æç”¨ï¼‰
            xml_coverage_file = self.reports_dir / "python_coverage.xml"
            if xml_coverage_file.exists():
                coverage_data["xml_report_available"] = True
            
            logger.info(f"Python coverage: {coverage_data['coverage_percentage']:.1f}%")
            
        except subprocess.TimeoutExpired:
            coverage_data["execution_status"] = "timeout"
        except Exception as e:
            coverage_data["execution_status"] = "error"
            coverage_data["error"] = str(e)
        
        return coverage_data
    
    def analyze_frontend_coverage(self) -> Dict[str, Any]:
        """ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰(React/TypeScript)ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ"""
        logger.info("âš›ï¸ Analyzing frontend test coverage...")
        
        frontend_coverage = {
            "vitest_coverage": {},
            "cypress_coverage": {},
            "combined_coverage": {},
            "execution_status": "unknown"
        }
        
        if not self.frontend_dir.exists():
            frontend_coverage["execution_status"] = "no_frontend"
            return frontend_coverage
        
        try:
            # Vitest ã‚«ãƒãƒ¬ãƒƒã‚¸å®Ÿè¡Œ
            vitest_cmd = ["npm", "run", "test:coverage"]
            
            vitest_result = subprocess.run(
                vitest_cmd, 
                cwd=self.frontend_dir,
                capture_output=True, text=True, timeout=300
            )
            
            frontend_coverage["vitest_coverage"] = {
                "exit_code": vitest_result.returncode,
                "executed": True
            }
            
            # Vitest coverageçµæœèª­ã¿è¾¼ã¿
            vitest_coverage_file = self.frontend_dir / "coverage" / "coverage-summary.json"
            if vitest_coverage_file.exists():
                with open(vitest_coverage_file) as f:
                    vitest_data = json.load(f)
                    frontend_coverage["vitest_coverage"]["results"] = vitest_data
            
            # Cypress ã‚«ãƒãƒ¬ãƒƒã‚¸å®Ÿè¡Œï¼ˆå¯èƒ½ã§ã‚ã‚Œã°ï¼‰
            try:
                cypress_cmd = ["npm", "run", "test:e2e", "--", "--coverage"]
                
                cypress_result = subprocess.run(
                    cypress_cmd,
                    cwd=self.frontend_dir, 
                    capture_output=True, text=True, timeout=300
                )
                
                frontend_coverage["cypress_coverage"] = {
                    "exit_code": cypress_result.returncode,
                    "executed": True
                }
                
            except Exception as e:
                frontend_coverage["cypress_coverage"] = {
                    "executed": False,
                    "error": str(e)
                }
            
            frontend_coverage["execution_status"] = "completed"
            
        except Exception as e:
            frontend_coverage["execution_status"] = "error"
            frontend_coverage["error"] = str(e)
        
        return frontend_coverage
    
    def analyze_powershell_coverage(self) -> Dict[str, Any]:
        """PowerShell ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ"""
        logger.info("âš¡ Analyzing PowerShell test coverage...")
        
        powershell_coverage = {
            "pester_coverage": {},
            "manual_analysis": {},
            "execution_status": "unknown"
        }
        
        try:
            # Pester ã‚«ãƒãƒ¬ãƒƒã‚¸å®Ÿè¡Œ
            pester_runner = self.project_root / "Tests" / "powershell_integration" / "Run-PesterTests.ps1"
            
            if pester_runner.exists():
                cmd = [
                    "pwsh", "-ExecutionPolicy", "Bypass",
                    "-File", str(pester_runner),
                    "-TestPath", str(self.project_root),
                    "-TestType", "All",
                    "-OutputPath", str(self.reports_dir)
                ]
                
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
                
                powershell_coverage["pester_coverage"] = {
                    "exit_code": result.returncode,
                    "executed": True,
                    "success": result.returncode == 0
                }
            
            # PowerShellãƒ•ã‚¡ã‚¤ãƒ«æ‰‹å‹•åˆ†æ
            ps_files = list(self.apps_dir.glob("*.ps1")) + list(self.scripts_dir.glob("**/*.ps1"))
            
            analyzed_files = []
            total_functions = 0
            
            for ps_file in ps_files:
                try:
                    content = ps_file.read_text(encoding='utf-8')
                    function_count = content.count("function ")
                    param_count = content.count("param(")
                    
                    analyzed_files.append({
                        "file": str(ps_file),
                        "functions": function_count,
                        "parameters": param_count,
                        "lines": len(content.splitlines())
                    })
                    
                    total_functions += function_count
                    
                except Exception as e:
                    logger.warning(f"Failed to analyze {ps_file}: {e}")
            
            powershell_coverage["manual_analysis"] = {
                "files_analyzed": len(analyzed_files),
                "total_functions": total_functions,
                "detailed_files": analyzed_files
            }
            
            powershell_coverage["execution_status"] = "completed"
            
        except Exception as e:
            powershell_coverage["execution_status"] = "error"
            powershell_coverage["error"] = str(e)
        
        return powershell_coverage
    
    def analyze_26_features_coverage(self) -> Dict[str, Any]:
        """26æ©Ÿèƒ½ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ"""
        logger.info("ğŸ¯ Analyzing 26 features coverage...")
        
        # 26æ©Ÿèƒ½å®šç¾©
        features_26 = {
            "regular_reports": [
                "daily_report", "weekly_report", "monthly_report", 
                "yearly_report", "test_execution"
            ],
            "analysis_reports": [
                "license_analysis", "usage_analysis", "performance_analysis",
                "security_analysis", "permission_audit"
            ],
            "entraid_management": [
                "user_list", "mfa_status", "conditional_access", "signin_logs"
            ],
            "exchange_management": [
                "mailbox_management", "mail_flow", "spam_protection", "delivery_analysis"
            ],
            "teams_management": [
                "teams_usage", "teams_settings", "meeting_quality", "teams_apps"
            ],
            "onedrive_management": [
                "storage_analysis", "sharing_analysis", "sync_errors", "external_sharing"
            ]
        }
        
        all_features = []
        for category_features in features_26.values():
            all_features.extend(category_features)
        
        # å„æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ
        features_coverage = {
            "total_features": len(all_features),
            "categories": len(features_26),
            "coverage_by_category": {},
            "coverage_by_feature": {},
            "overall_coverage": 0.0
        }
        
        covered_features = 0
        
        for category, features in features_26.items():
            category_coverage = {
                "total": len(features),
                "covered": 0,
                "features": {}
            }
            
            for feature in features:
                # å„æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆå­˜åœ¨ç¢ºèª
                feature_covered = self._check_feature_test_coverage(feature)
                category_coverage["features"][feature] = feature_covered
                
                if feature_covered["has_tests"]:
                    category_coverage["covered"] += 1
                    covered_features += 1
            
            category_coverage["coverage_percentage"] = (
                category_coverage["covered"] / category_coverage["total"] * 100
            )
            
            features_coverage["coverage_by_category"][category] = category_coverage
        
        features_coverage["overall_coverage"] = (covered_features / len(all_features)) * 100
        
        return features_coverage
    
    def _check_feature_test_coverage(self, feature: str) -> Dict[str, Any]:
        """å€‹åˆ¥æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯"""
        coverage_info = {
            "feature": feature,
            "has_tests": False,
            "test_files": [],
            "test_functions": 0,
            "implementations": []
        }
        
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        test_patterns = [
            f"**/test_{feature}*.py",
            f"**/test_*{feature}*.py",
            f"**/{feature}_test.py",
            f"**/*{feature}*.spec.ts",
            f"**/*{feature}*.cy.ts"
        ]
        
        for pattern in test_patterns:
            test_files = list(self.project_root.glob(pattern))
            for test_file in test_files:
                coverage_info["test_files"].append(str(test_file))
                
                # ãƒ†ã‚¹ãƒˆé–¢æ•°æ•°ã‚«ã‚¦ãƒ³ãƒˆ
                try:
                    content = test_file.read_text(encoding='utf-8')
                    if test_file.suffix == ".py":
                        coverage_info["test_functions"] += content.count("def test_")
                    elif test_file.suffix in [".ts", ".js"]:
                        coverage_info["test_functions"] += content.count("test(") + content.count("it(")
                except Exception:
                    pass
        
        # å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        impl_patterns = [
            f"**/src/**/*{feature}*.py",
            f"**/Apps/*{feature}*.ps1",
            f"**/Scripts/**/*{feature}*.ps1",
            f"**/frontend/src/**/*{feature}*.ts",
            f"**/frontend/src/**/*{feature}*.tsx"
        ]
        
        for pattern in impl_patterns:
            impl_files = list(self.project_root.glob(pattern))
            for impl_file in impl_files:
                coverage_info["implementations"].append(str(impl_file))
        
        coverage_info["has_tests"] = len(coverage_info["test_files"]) > 0
        
        return coverage_info
    
    def calculate_quality_metrics(self) -> Dict[str, Any]:
        """å“è³ªæŒ‡æ¨™è¨ˆç®—"""
        logger.info("ğŸ“Š Calculating quality metrics...")
        
        quality_metrics = {
            "complexity": self._calculate_complexity(),
            "maintainability": self._calculate_maintainability(),
            "test_quality": self._calculate_test_quality(),
            "performance": self._calculate_performance_metrics(),
            "overall_score": 0.0
        }
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        scores = [
            quality_metrics["complexity"].get("score", 0),
            quality_metrics["maintainability"].get("score", 0), 
            quality_metrics["test_quality"].get("score", 0),
            quality_metrics["performance"].get("score", 0)
        ]
        
        quality_metrics["overall_score"] = sum(scores) / len(scores) if scores else 0.0
        
        return quality_metrics
    
    def _calculate_complexity(self) -> Dict[str, Any]:
        """è¤‡é›‘åº¦è¨ˆç®—"""
        complexity_data = {
            "average_complexity": 0.0,
            "max_complexity": 0.0,
            "files_analyzed": 0,
            "score": 100.0
        }
        
        try:
            # radonå®Ÿè¡Œï¼ˆè¤‡é›‘åº¦æ¸¬å®šãƒ„ãƒ¼ãƒ«ï¼‰
            result = subprocess.run(
                ["python", "-m", "radon", "cc", str(self.src_dir), "-j"],
                capture_output=True, text=True, timeout=60
            )
            
            if result.returncode == 0:
                radon_data = json.loads(result.stdout)
                complexities = []
                
                for file_data in radon_data.values():
                    for item in file_data:
                        if isinstance(item, dict) and "complexity" in item:
                            complexities.append(item["complexity"])
                
                if complexities:
                    complexity_data["average_complexity"] = sum(complexities) / len(complexities)
                    complexity_data["max_complexity"] = max(complexities)
                    complexity_data["files_analyzed"] = len(radon_data)
                    
                    # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆä½ã„è¤‡é›‘åº¦ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
                    avg_complexity = complexity_data["average_complexity"]
                    complexity_data["score"] = max(0, 100 - (avg_complexity * 10))
                
        except Exception as e:
            logger.warning(f"Complexity analysis failed: {e}")
        
        return complexity_data
    
    def _calculate_maintainability(self) -> Dict[str, Any]:
        """ä¿å®ˆæ€§æŒ‡æ¨™è¨ˆç®—"""
        maintainability_data = {
            "maintainability_index": 0.0,
            "files_analyzed": 0,
            "score": 0.0
        }
        
        try:
            # radonå®Ÿè¡Œï¼ˆä¿å®ˆæ€§æŒ‡æ¨™æ¸¬å®šï¼‰
            result = subprocess.run(
                ["python", "-m", "radon", "mi", str(self.src_dir), "-j"],
                capture_output=True, text=True, timeout=60
            )
            
            if result.returncode == 0:
                mi_data = json.loads(result.stdout)
                mi_scores = []
                
                for file_path, mi_value in mi_data.items():
                    if isinstance(mi_value, (int, float)):
                        mi_scores.append(mi_value)
                
                if mi_scores:
                    maintainability_data["maintainability_index"] = sum(mi_scores) / len(mi_scores)
                    maintainability_data["files_analyzed"] = len(mi_scores)
                    maintainability_data["score"] = maintainability_data["maintainability_index"]
                
        except Exception as e:
            logger.warning(f"Maintainability analysis failed: {e}")
        
        return maintainability_data
    
    def _calculate_test_quality(self) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆå“è³ªè¨ˆç®—"""
        test_quality_data = {
            "test_count": 0,
            "assertion_count": 0,
            "test_files": 0,
            "score": 0.0
        }
        
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
        test_files = list(self.tests_dir.glob("**/test_*.py"))
        test_count = 0
        assertion_count = 0
        
        for test_file in test_files:
            try:
                content = test_file.read_text(encoding='utf-8')
                test_count += content.count("def test_")
                assertion_count += content.count("assert ")
                
            except Exception:
                pass
        
        test_quality_data["test_count"] = test_count
        test_quality_data["assertion_count"] = assertion_count
        test_quality_data["test_files"] = len(test_files)
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        if test_count > 0:
            assertions_per_test = assertion_count / test_count
            test_quality_data["score"] = min(100, assertions_per_test * 20)  # 5 assertions per test = 100 score
        
        return test_quality_data
    
    def _calculate_performance_metrics(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™è¨ˆç®—"""
        performance_data = {
            "test_execution_time": 0.0,
            "memory_usage": 0.0,
            "score": 80.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚³ã‚¢
        }
        
        # ç°¡å˜ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        try:
            start_time = datetime.now()
            
            result = subprocess.run(
                ["python", "-m", "pytest", str(self.tests_dir), "-q", "--tb=no", "--maxfail=1"],
                capture_output=True, text=True, timeout=120
            )
            
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            
            performance_data["test_execution_time"] = execution_time
            
            # å®Ÿè¡Œæ™‚é–“ã«åŸºã¥ãã‚¹ã‚³ã‚¢ï¼ˆçŸ­ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
            if execution_time < 30:
                performance_data["score"] = 100
            elif execution_time < 60:
                performance_data["score"] = 80
            elif execution_time < 120:
                performance_data["score"] = 60
            else:
                performance_data["score"] = 40
                
        except Exception as e:
            logger.warning(f"Performance analysis failed: {e}")
        
        return performance_data
    
    def generate_coverage_improvement_suggestions(self, coverage_results: Dict[str, Any]) -> List[str]:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸æ”¹å–„ææ¡ˆç”Ÿæˆ"""
        logger.info("ğŸ’¡ Generating coverage improvement suggestions...")
        
        suggestions = []
        
        # Python ã‚«ãƒãƒ¬ãƒƒã‚¸
        python_coverage = coverage_results.get("python_coverage", {}).get("coverage_percentage", 0)
        if python_coverage < self.coverage_targets["python"]:
            suggestions.append(
                f"Python ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’ {python_coverage:.1f}% ã‹ã‚‰ {self.coverage_targets['python']}% ã«å‘ä¸Šã•ã›ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
            )
            suggestions.append("æœªãƒ†ã‚¹ãƒˆã®é–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹ã«å¯¾ã—ã¦ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
        
        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ ã‚«ãƒãƒ¬ãƒƒã‚¸
        frontend_coverage = coverage_results.get("frontend_coverage", {})
        if frontend_coverage.get("execution_status") == "completed":
            suggestions.append("ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ†ã‚¹ãƒˆã®æ‹¡å……ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        
        # 26æ©Ÿèƒ½ã‚«ãƒãƒ¬ãƒƒã‚¸
        features_coverage = coverage_results.get("features_26_coverage", {})
        uncovered_features = []
        
        for category, data in features_coverage.get("coverage_by_category", {}).items():
            for feature, feature_data in data.get("features", {}).items():
                if not feature_data.get("has_tests"):
                    uncovered_features.append(feature)
        
        if uncovered_features:
            suggestions.append(f"ä»¥ä¸‹ã®{len(uncovered_features)}æ©Ÿèƒ½ã«ãƒ†ã‚¹ãƒˆãŒä¸è¶³ã—ã¦ã„ã¾ã™: {', '.join(uncovered_features[:5])}")
            suggestions.append("å„æ©Ÿèƒ½ã«E2Eãƒ†ã‚¹ãƒˆã¨çµ±åˆãƒ†ã‚¹ãƒˆã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
        
        # å“è³ªæŒ‡æ¨™
        quality_metrics = coverage_results.get("quality_metrics", {})
        overall_score = quality_metrics.get("overall_score", 0)
        
        if overall_score < 80:
            suggestions.append(f"å“è³ªã‚¹ã‚³ã‚¢ {overall_score:.1f} ã‚’80ä»¥ä¸Šã«å‘ä¸Šã•ã›ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
            
            complexity = quality_metrics.get("complexity", {})
            if complexity.get("average_complexity", 0) > 8:
                suggestions.append("è¤‡é›‘åº¦ã®é«˜ã„é–¢æ•°ã‚’ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã—ã¦ãã ã•ã„ã€‚")
        
        # å…·ä½“çš„ãªæ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        suggestions.extend([
            "pytest --cov ã§è©³ç´°ãªã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
            "æœªã‚«ãƒãƒ¼ã®åˆ†å²æ¡ä»¶ã«å¯¾ã—ã¦ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚",
            "ãƒ¢ãƒƒã‚¯ãƒ»ã‚¹ã‚¿ãƒ–ã‚’æ´»ç”¨ã—ã¦å¤–éƒ¨ä¾å­˜ã‚’åˆ†é›¢ã—ãŸãƒ†ã‚¹ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚",
            "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå™¨ã‚’ä½¿ç”¨ã—ã¦ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆã‚’å……å®Ÿã•ã›ã¦ãã ã•ã„ã€‚"
        ])
        
        return suggestions
    
    def run_comprehensive_coverage_analysis(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æå®Ÿè¡Œ"""
        logger.info("ğŸš€ Running comprehensive coverage analysis...")
        
        # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹
        source_files = self.discover_all_source_files()
        
        # Python ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ
        python_coverage = self.analyze_python_coverage()
        
        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ
        frontend_coverage = self.analyze_frontend_coverage()
        
        # PowerShell ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ
        powershell_coverage = self.analyze_powershell_coverage()
        
        # 26æ©Ÿèƒ½ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ
        features_26_coverage = self.analyze_26_features_coverage()
        
        # å“è³ªæŒ‡æ¨™è¨ˆç®—
        quality_metrics = self.calculate_quality_metrics()
        
        # çµ±åˆçµæœ
        comprehensive_results = {
            "timestamp": self.timestamp,
            "project_root": str(self.project_root),
            "analysis_phase": "comprehensive_coverage",
            "coverage_targets": self.coverage_targets,
            "source_files": source_files,
            "python_coverage": python_coverage,
            "frontend_coverage": frontend_coverage,
            "powershell_coverage": powershell_coverage,
            "features_26_coverage": features_26_coverage,
            "quality_metrics": quality_metrics,
            "overall_assessment": self._generate_overall_assessment(
                python_coverage, frontend_coverage, features_26_coverage, quality_metrics
            )
        }
        
        # æ”¹å–„ææ¡ˆç”Ÿæˆ
        comprehensive_results["improvement_suggestions"] = self.generate_coverage_improvement_suggestions(
            comprehensive_results
        )
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        final_report = self.reports_dir / f"comprehensive_coverage_report_{self.timestamp}.json"
        with open(final_report, 'w') as f:
            json.dump(comprehensive_results, f, indent=2)
        
        # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        html_report = self._generate_html_report(comprehensive_results)
        html_report_path = self.reports_dir / f"comprehensive_coverage_report_{self.timestamp}.html"
        with open(html_report_path, 'w', encoding='utf-8') as f:
            f.write(html_report)
        
        logger.info(f"âœ… Comprehensive coverage analysis completed!")
        logger.info(f"ğŸ“„ JSON Report: {final_report}")
        logger.info(f"ğŸŒ HTML Report: {html_report_path}")
        
        return comprehensive_results
    
    def _generate_overall_assessment(self, python_cov, frontend_cov, features_cov, quality) -> Dict[str, Any]:
        """ç·åˆè©•ä¾¡ç”Ÿæˆ"""
        assessment = {
            "coverage_score": 0.0,
            "quality_score": 0.0,
            "features_completeness": 0.0,
            "overall_grade": "F",
            "target_achievement": False
        }
        
        # ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚¹ã‚³ã‚¢
        python_score = python_cov.get("coverage_percentage", 0)
        features_score = features_cov.get("overall_coverage", 0)
        coverage_score = (python_score * 0.6 + features_score * 0.4)  # é‡ã¿ä»˜ã‘å¹³å‡
        
        assessment["coverage_score"] = coverage_score
        assessment["quality_score"] = quality.get("overall_score", 0)
        assessment["features_completeness"] = features_score
        
        # ç·åˆè©•ä¾¡
        overall_score = (coverage_score * 0.5 + assessment["quality_score"] * 0.3 + features_score * 0.2)
        
        if overall_score >= 90:
            assessment["overall_grade"] = "A"
        elif overall_score >= 80:
            assessment["overall_grade"] = "B"
        elif overall_score >= 70:
            assessment["overall_grade"] = "C"
        elif overall_score >= 60:
            assessment["overall_grade"] = "D"
        else:
            assessment["overall_grade"] = "F"
        
        assessment["target_achievement"] = coverage_score >= self.coverage_targets["overall"]
        
        return assessment
    
    def _generate_html_report(self, results: Dict[str, Any]) -> str:
        """HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        html_content = f'''<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆ - Microsoft 365 Management Tools</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 0; padding: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; margin-bottom: 30px; border-bottom: 2px solid #0078d4; padding-bottom: 20px; }}
        .metric-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 20px 0; }}
        .metric-card {{ background: #f8f9fa; padding: 20px; border-radius: 8px; border-left: 4px solid #0078d4; }}
        .metric-value {{ font-size: 2em; font-weight: bold; color: #0078d4; }}
        .progress-bar {{ width: 100%; height: 20px; background: #e9ecef; border-radius: 10px; overflow: hidden; margin: 10px 0; }}
        .progress-fill {{ height: 100%; background: linear-gradient(90deg, #ff6b6b, #feca57, #48dbfb, #0be881); transition: width 0.3s; }}
        .grade-a {{ color: #0be881; }}
        .grade-b {{ color: #48dbfb; }}
        .grade-c {{ color: #feca57; }}
        .grade-d {{ color: #ff9ff3; }}
        .grade-f {{ color: #ff6b6b; }}
        .suggestions {{ background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 5px; padding: 15px; margin: 20px 0; }}
        .feature-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 10px; }}
        .feature-item {{ padding: 10px; border-radius: 5px; text-align: center; }}
        .covered {{ background: #d4edda; color: #155724; }}
        .uncovered {{ background: #f8d7da; color: #721c24; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆ</h1>
            <h2>Microsoft 365 Management Tools</h2>
            <p>ç”Ÿæˆæ—¥æ™‚: {results['timestamp']}</p>
        </div>
        
        <div class="metric-grid">
            <div class="metric-card">
                <h3>ğŸ¯ ç·åˆã‚«ãƒãƒ¬ãƒƒã‚¸</h3>
                <div class="metric-value">{results['overall_assessment']['coverage_score']:.1f}%</div>
                <div class="progress-bar">
                    <div class="progress-fill" style="width: {results['overall_assessment']['coverage_score']}%"></div>
                </div>
                <p>ç›®æ¨™: {results['coverage_targets']['overall']}%</p>
            </div>
            
            <div class="metric-card">
                <h3>ğŸ Python ã‚«ãƒãƒ¬ãƒƒã‚¸</h3>
                <div class="metric-value">{results['python_coverage']['coverage_percentage']:.1f}%</div>
                <div class="progress-bar">
                    <div class="progress-fill" style="width: {results['python_coverage']['coverage_percentage']}%"></div>
                </div>
                <p>å®Ÿè¡Œãƒ©ã‚¤ãƒ³: {results['python_coverage']['lines_covered']}/{results['python_coverage']['lines_total']}</p>
            </div>
            
            <div class="metric-card">
                <h3>ğŸ¯ 26æ©Ÿèƒ½ã‚«ãƒãƒ¬ãƒƒã‚¸</h3>
                <div class="metric-value">{results['features_26_coverage']['overall_coverage']:.1f}%</div>
                <div class="progress-bar">
                    <div class="progress-fill" style="width: {results['features_26_coverage']['overall_coverage']}%"></div>
                </div>
                <p>æ©Ÿèƒ½æ•°: {results['features_26_coverage']['total_features']}</p>
            </div>
            
            <div class="metric-card">
                <h3>ğŸ“ˆ å“è³ªã‚¹ã‚³ã‚¢</h3>
                <div class="metric-value grade-{results['overall_assessment']['overall_grade'].lower()}">{results['overall_assessment']['overall_grade']}</div>
                <p>ç·åˆã‚¹ã‚³ã‚¢: {results['quality_metrics']['overall_score']:.1f}</p>
            </div>
        </div>
        
        <h3>ğŸ¯ 26æ©Ÿèƒ½ã‚«ãƒãƒ¬ãƒƒã‚¸è©³ç´°</h3>
        <div class="feature-grid">
'''
        
        # 26æ©Ÿèƒ½è©³ç´°
        for category, data in results['features_26_coverage']['coverage_by_category'].items():
            for feature, feature_data in data['features'].items():
                status_class = "covered" if feature_data['has_tests'] else "uncovered"
                status_text = "âœ… ãƒ†ã‚¹ãƒˆæ¸ˆ" if feature_data['has_tests'] else "âŒ æœªãƒ†ã‚¹ãƒˆ"
                html_content += f'''
            <div class="feature-item {status_class}">
                <strong>{feature}</strong><br>
                {status_text}
            </div>'''
        
        html_content += f'''
        </div>
        
        <h3>ğŸ’¡ æ”¹å–„ææ¡ˆ</h3>
        <div class="suggestions">
            <ul>
'''
        
        for suggestion in results['improvement_suggestions'][:10]:
            html_content += f'<li>{suggestion}</li>'
        
        html_content += '''
            </ul>
        </div>
        
        <h3>ğŸ“Š è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹</h3>
        <div class="metric-grid">
            <div class="metric-card">
                <h4>è¤‡é›‘åº¦</h4>
                <p>å¹³å‡: {:.1f}</p>
                <p>æœ€å¤§: {:.1f}</p>
            </div>
            <div class="metric-card">
                <h4>ä¿å®ˆæ€§</h4>
                <p>æŒ‡æ¨™: {:.1f}</p>
            </div>
            <div class="metric-card">
                <h4>ãƒ†ã‚¹ãƒˆå“è³ª</h4>
                <p>ãƒ†ã‚¹ãƒˆæ•°: {}</p>
                <p>ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³æ•°: {}</p>
            </div>
            <div class="metric-card">
                <h4>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹</h4>
                <p>å®Ÿè¡Œæ™‚é–“: {:.1f}ç§’</p>
            </div>
        </div>
    </div>
</body>
</html>'''.format(
            results['quality_metrics']['complexity']['average_complexity'],
            results['quality_metrics']['complexity']['max_complexity'],
            results['quality_metrics']['maintainability']['maintainability_index'],
            results['quality_metrics']['test_quality']['test_count'],
            results['quality_metrics']['test_quality']['assertion_count'],
            results['quality_metrics']['performance']['test_execution_time']
        )
        
        return html_content


# pytestçµ±åˆç”¨ãƒ†ã‚¹ãƒˆé–¢æ•°
@pytest.mark.coverage
@pytest.mark.quality
def test_coverage_analyzer_setup():
    """ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æå™¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ"""
    analyzer = ComprehensiveCoverageAnalyzer()
    
    assert analyzer.coverage_targets["overall"] == 90.0
    assert analyzer.coverage_targets["26_features"] == 100.0
    assert len(analyzer.quality_thresholds) >= 4


@pytest.mark.coverage
@pytest.mark.slow
def test_python_coverage_analysis():
    """Python ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æãƒ†ã‚¹ãƒˆ"""
    analyzer = ComprehensiveCoverageAnalyzer()
    
    coverage_result = analyzer.analyze_python_coverage()
    assert coverage_result["execution_status"] in ["completed", "timeout", "error"]
    
    if coverage_result["execution_status"] == "completed":
        assert isinstance(coverage_result["coverage_percentage"], float)
        assert coverage_result["coverage_percentage"] >= 0


@pytest.mark.coverage
@pytest.mark.features_26
def test_26_features_coverage_analysis():
    """26æ©Ÿèƒ½ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æãƒ†ã‚¹ãƒˆ"""
    analyzer = ComprehensiveCoverageAnalyzer()
    
    features_coverage = analyzer.analyze_26_features_coverage()
    assert features_coverage["total_features"] == 26
    assert features_coverage["categories"] == 6
    assert "coverage_by_category" in features_coverage


@pytest.mark.coverage
@pytest.mark.quality
def test_quality_metrics_calculation():
    """å“è³ªæŒ‡æ¨™è¨ˆç®—ãƒ†ã‚¹ãƒˆ"""
    analyzer = ComprehensiveCoverageAnalyzer()
    
    quality_metrics = analyzer.calculate_quality_metrics()
    assert "complexity" in quality_metrics
    assert "maintainability" in quality_metrics
    assert "test_quality" in quality_metrics
    assert "performance" in quality_metrics


@pytest.mark.coverage
@pytest.mark.slow
def test_comprehensive_coverage_analysis():
    """åŒ…æ‹¬çš„ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æãƒ†ã‚¹ãƒˆ"""
    analyzer = ComprehensiveCoverageAnalyzer()
    
    results = analyzer.run_comprehensive_coverage_analysis()
    assert results["analysis_phase"] == "comprehensive_coverage"
    assert "overall_assessment" in results
    assert "improvement_suggestions" in results


if __name__ == "__main__":
    # ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å®Ÿè¡Œ
    analyzer = ComprehensiveCoverageAnalyzer()
    results = analyzer.run_comprehensive_coverage_analysis()
    
    print("\n" + "="*60)
    print("ğŸ“Š COMPREHENSIVE COVERAGE ANALYSIS RESULTS")
    print("="*60)
    print(f"Overall Coverage: {results['overall_assessment']['coverage_score']:.1f}%")
    print(f"Python Coverage: {results['python_coverage']['coverage_percentage']:.1f}%")
    print(f"26 Features Coverage: {results['features_26_coverage']['overall_coverage']:.1f}%")
    print(f"Quality Grade: {results['overall_assessment']['overall_grade']}")
    print(f"Target Achievement: {results['overall_assessment']['target_achievement']}")
    print("="*60)