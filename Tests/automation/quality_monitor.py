"""
å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è‡ªå‹•ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

Pythonç§»è¡Œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç”¨ã®å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è‡ªå‹•åé›†ãƒ»ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
4æ™‚é–“ã”ã¨ã®è‡ªå‹•å“è³ªãƒã‚§ãƒƒã‚¯ã€ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ä»˜ã
"""

import subprocess
import json
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
import os
import sys

class QualityMetricsMonitor:
    """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è‡ªå‹•ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, project_root: str = None):
        """
        åˆæœŸåŒ–
        
        Args:
            project_root: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹
        """
        self.project_root = Path(project_root) if project_root else Path(__file__).parent.parent.parent
        self.metrics_path = self.project_root / "reports" / "progress" / "quality"
        self.metrics_path.mkdir(parents=True, exist_ok=True)
        
        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸºæº–ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        self.escalation_rules = self._load_escalation_rules()
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
        self.log_file = self.project_root / "logs" / "quality_monitor.log"
        self.log_file.parent.mkdir(exist_ok=True)
    
    def _load_escalation_rules(self) -> Dict[str, Any]:
        """ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸºæº–ã®èª­ã¿è¾¼ã¿"""
        rules_file = self.project_root / "Config" / "escalation_rules.yml"
        
        if not rules_file.exists():
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸºæº–
            return {
                "escalation_criteria": {
                    "immediate": {
                        "test_coverage_below": 85,
                        "build_failures_consecutive": 3,
                        "repair_loops_exceed": 7,
                        "api_response_time_over": 3.0
                    },
                    "warning": {
                        "test_coverage_below": 88,
                        "repair_loops_exceed": 5,
                        "progress_completion_below": 80
                    }
                }
            }
        
        try:
            with open(rules_file, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            self._log_error(f"ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸºæº–èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return {}
    
    def run_automated_checks(self) -> Dict[str, Any]:
        """4æ™‚é–“ã”ã¨ã®è‡ªå‹•å“è³ªãƒã‚§ãƒƒã‚¯"""
        self._log_info("è‡ªå‹•å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹")
        
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "developer": "tester",
            "project_root": str(self.project_root),
            "quality_metrics": {
                "test_results": self.run_all_test_suites(),
                "coverage_report": self.generate_coverage_report(),
                "regression_status": self.check_regression_tests(),
                "compatibility_matrix": self.test_compatibility(),
                "code_quality": self.analyze_code_quality()
            }
        }
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä¿å­˜
        self._save_metrics(metrics)
        
        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®š
        self.check_escalation_criteria(metrics)
        
        self._log_info("è‡ªå‹•å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº†")
        return metrics
    
    def run_all_test_suites(self) -> Dict[str, Any]:
        """å…¨ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ"""
        self._log_info("ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œé–‹å§‹")
        
        test_results = {}
        
        # Pythonå˜ä½“ãƒ†ã‚¹ãƒˆ
        test_results["unit_tests"] = self._run_python_unit_tests()
        
        # çµ±åˆãƒ†ã‚¹ãƒˆ
        test_results["integration_tests"] = self._run_integration_tests()
        
        # API ãƒ†ã‚¹ãƒˆ
        test_results["api_tests"] = self._run_api_tests()
        
        # GUI ãƒ†ã‚¹ãƒˆ
        test_results["gui_tests"] = self._run_gui_tests()
        
        # PowerShelläº’æ›æ€§ãƒ†ã‚¹ãƒˆ
        test_results["compatibility"] = self._run_powershell_compatibility_tests()
        
        # å…¨ä½“ã‚µãƒãƒªãƒ¼
        test_results["summary"] = self._calculate_test_summary(test_results)
        
        return test_results
    
    def _run_python_unit_tests(self) -> Dict[str, Any]:
        """Pythonå˜ä½“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        try:
            result = subprocess.run(
                [sys.executable, "-m", "pytest", "tests/unit/", "-v", "--tb=short", "--json-report", "--json-report-file=reports/progress/unit_test_results.json"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300
            )
            
            return {
                "passed": result.returncode == 0,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": result.returncode
            }
        except subprocess.TimeoutExpired:
            return {"passed": False, "error": "ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ", "returncode": -1}
        except Exception as e:
            return {"passed": False, "error": str(e), "returncode": -1}
    
    def _run_integration_tests(self) -> Dict[str, Any]:
        """çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        try:
            result = subprocess.run(
                [sys.executable, "-m", "pytest", "tests/integration/", "-v", "--tb=short"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=600
            )
            
            return {
                "passed": result.returncode == 0,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": result.returncode
            }
        except Exception as e:
            return {"passed": False, "error": str(e), "returncode": -1}
    
    def _run_api_tests(self) -> Dict[str, Any]:
        """API ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        try:
            result = subprocess.run(
                [sys.executable, "-m", "pytest", "tests/api/", "-v", "--tb=short"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300
            )
            
            return {
                "passed": result.returncode == 0,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": result.returncode
            }
        except Exception as e:
            return {"passed": False, "error": str(e), "returncode": -1}
    
    def _run_gui_tests(self) -> Dict[str, Any]:
        """GUI ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        try:
            result = subprocess.run(
                [sys.executable, "-m", "pytest", "tests/unit/test_gui_components.py", "-v", "--tb=short"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300
            )
            
            return {
                "passed": result.returncode == 0,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": result.returncode
            }
        except Exception as e:
            return {"passed": False, "error": str(e), "returncode": -1}
    
    def _run_powershell_compatibility_tests(self) -> Dict[str, Any]:
        """PowerShelläº’æ›æ€§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        try:
            # PowerShelläº’æ›æ€§ãƒ†ã‚¹ãƒˆ
            result = subprocess.run(
                [sys.executable, "-m", "pytest", "tests/compatibility/", "-v", "--tb=short"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=600
            )
            
            return {
                "passed": result.returncode == 0,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": result.returncode
            }
        except Exception as e:
            return {"passed": False, "error": str(e), "returncode": -1}
    
    def _calculate_test_summary(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼è¨ˆç®—"""
        total_suites = len([k for k in test_results.keys() if k != "summary"])
        passed_suites = len([k for k, v in test_results.items() if k != "summary" and v.get("passed", False)])
        
        return {
            "total_suites": total_suites,
            "passed_suites": passed_suites,
            "success_rate": (passed_suites / total_suites * 100) if total_suites > 0 else 0
        }
    
    def generate_coverage_report(self) -> Dict[str, Any]:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        try:
            # pytestã‚«ãƒãƒ¬ãƒƒã‚¸å®Ÿè¡Œ
            result = subprocess.run(
                [sys.executable, "-m", "pytest", "--cov=src", "--cov-report=json", "--cov-report=term"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=600
            )
            
            # JSONã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿
            coverage_file = self.project_root / "coverage.json"
            if coverage_file.exists():
                with open(coverage_file, 'r') as f:
                    coverage_data = json.load(f)
                    
                total_coverage = coverage_data.get("totals", {}).get("percent_covered", 0)
                
                return {
                    "total": total_coverage,
                    "detailed": coverage_data,
                    "status": "success"
                }
            else:
                return {
                    "total": 0,
                    "error": "ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                    "status": "failed"
                }
                
        except Exception as e:
            return {
                "total": 0,
                "error": str(e),
                "status": "failed"
            }
    
    def check_regression_tests(self) -> Dict[str, Any]:
        """ãƒ¬ã‚°ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯"""
        try:
            # ãƒ¬ã‚°ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
            regression_dir = self.project_root / "tests" / "regression"
            
            if not regression_dir.exists():
                return {
                    "status": "not_configured",
                    "message": "ãƒ¬ã‚°ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“"
                }
            
            # ãƒ¬ã‚°ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            result = subprocess.run(
                [sys.executable, "-m", "pytest", str(regression_dir), "-v"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300
            )
            
            return {
                "status": "completed" if result.returncode == 0 else "failed",
                "passed": result.returncode == 0,
                "stdout": result.stdout,
                "stderr": result.stderr
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    def test_compatibility(self) -> Dict[str, Any]:
        """äº’æ›æ€§ãƒ†ã‚¹ãƒˆãƒãƒˆãƒªã‚¯ã‚¹"""
        compatibility_results = {}
        
        # PowerShell ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§
        compatibility_results["powershell_versions"] = self._test_powershell_versions()
        
        # Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§
        compatibility_results["python_versions"] = self._test_python_versions()
        
        # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ äº’æ›æ€§
        compatibility_results["platforms"] = self._test_platform_compatibility()
        
        return compatibility_results
    
    def _test_powershell_versions(self) -> Dict[str, Any]:
        """PowerShell ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§ãƒ†ã‚¹ãƒˆ"""
        try:
            # PowerShell 5.1 ãƒ†ã‚¹ãƒˆ
            ps51_result = subprocess.run(
                ["powershell", "-Command", "Get-Host | Select-Object Version"],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            # PowerShell 7.x ãƒ†ã‚¹ãƒˆ
            ps7_result = subprocess.run(
                ["pwsh", "-Command", "Get-Host | Select-Object Version"],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            return {
                "powershell_5_1": ps51_result.returncode == 0,
                "powershell_7_x": ps7_result.returncode == 0,
                "overall_status": "compatible" if ps51_result.returncode == 0 and ps7_result.returncode == 0 else "issues"
            }
        except Exception as e:
            return {"error": str(e), "overall_status": "error"}
    
    def _test_python_versions(self) -> Dict[str, Any]:
        """Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§ãƒ†ã‚¹ãƒˆ"""
        try:
            python_version = sys.version_info
            
            # Python 3.9+ ãƒã‚§ãƒƒã‚¯
            min_version_ok = python_version >= (3, 9)
            
            # Python 3.11 æ¨å¥¨ãƒã‚§ãƒƒã‚¯
            recommended_version = python_version >= (3, 11)
            
            return {
                "current_version": f"{python_version.major}.{python_version.minor}.{python_version.micro}",
                "min_version_ok": min_version_ok,
                "recommended_version": recommended_version,
                "overall_status": "compatible" if min_version_ok else "incompatible"
            }
        except Exception as e:
            return {"error": str(e), "overall_status": "error"}
    
    def _test_platform_compatibility(self) -> Dict[str, Any]:
        """ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ äº’æ›æ€§ãƒ†ã‚¹ãƒˆ"""
        import platform
        
        try:
            return {
                "platform": platform.system(),
                "architecture": platform.machine(),
                "python_implementation": platform.python_implementation(),
                "overall_status": "compatible"
            }
        except Exception as e:
            return {"error": str(e), "overall_status": "error"}
    
    def analyze_code_quality(self) -> Dict[str, Any]:
        """ã‚³ãƒ¼ãƒ‰å“è³ªåˆ†æ"""
        quality_metrics = {}
        
        # flake8 ã«ã‚ˆã‚‹é™çš„è§£æ
        quality_metrics["flake8"] = self._run_flake8_analysis()
        
        # pylint ã«ã‚ˆã‚‹è©³ç´°è§£æ
        quality_metrics["pylint"] = self._run_pylint_analysis()
        
        # è¤‡é›‘åº¦åˆ†æ
        quality_metrics["complexity"] = self._analyze_complexity()
        
        return quality_metrics
    
    def _run_flake8_analysis(self) -> Dict[str, Any]:
        """flake8 é™çš„è§£æ"""
        try:
            result = subprocess.run(
                ["flake8", "src/", "--statistics", "--count"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=120
            )
            
            return {
                "passed": result.returncode == 0,
                "issues_count": result.stdout.count('\n') if result.stdout else 0,
                "output": result.stdout
            }
        except Exception as e:
            return {"error": str(e), "passed": False}
    
    def _run_pylint_analysis(self) -> Dict[str, Any]:
        """pylint è©³ç´°è§£æ"""
        try:
            result = subprocess.run(
                ["pylint", "src/", "--output-format=json"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300
            )
            
            return {
                "passed": result.returncode == 0,
                "output": result.stdout
            }
        except Exception as e:
            return {"error": str(e), "passed": False}
    
    def _analyze_complexity(self) -> Dict[str, Any]:
        """è¤‡é›‘åº¦åˆ†æ"""
        try:
            # radon ã«ã‚ˆã‚‹è¤‡é›‘åº¦åˆ†æ
            result = subprocess.run(
                ["radon", "cc", "src/", "-j"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=120
            )
            
            if result.returncode == 0:
                try:
                    complexity_data = json.loads(result.stdout)
                    return {
                        "status": "success",
                        "data": complexity_data
                    }
                except json.JSONDecodeError:
                    return {"status": "parse_error", "raw_output": result.stdout}
            else:
                return {"status": "failed", "stderr": result.stderr}
                
        except Exception as e:
            return {"error": str(e), "status": "error"}
    
    def check_escalation_criteria(self, metrics: Dict[str, Any]) -> None:
        """ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸºæº–ãƒã‚§ãƒƒã‚¯"""
        try:
            criteria = self.escalation_rules.get("escalation_criteria", {})
            quality_metrics = metrics.get("quality_metrics", {})
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯
            coverage_report = quality_metrics.get("coverage_report", {})
            coverage = coverage_report.get("total", 0)
            
            # å³æ™‚ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸºæº–
            immediate_criteria = criteria.get("immediate", {})
            if coverage < immediate_criteria.get("test_coverage_below", 85):
                self.escalate_to_architect(
                    f"CRITICAL: ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãŒ{coverage}%ã§åŸºæº–å€¤{immediate_criteria.get('test_coverage_below', 85)}%ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™",
                    metrics
                )
            
            # è­¦å‘Šã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸºæº–
            warning_criteria = criteria.get("warning", {})
            if coverage < warning_criteria.get("test_coverage_below", 88):
                self.escalate_to_architect(
                    f"WARNING: ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãŒ{coverage}%ã§è­¦å‘ŠåŸºæº–{warning_criteria.get('test_coverage_below', 88)}%ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™",
                    metrics
                )
            
            # ãƒ†ã‚¹ãƒˆå¤±æ•—é€£ç¶šãƒã‚§ãƒƒã‚¯
            test_results = quality_metrics.get("test_results", {})
            test_summary = test_results.get("summary", {})
            
            if test_summary.get("success_rate", 100) < 50:
                self.escalate_to_architect(
                    f"CRITICAL: ãƒ†ã‚¹ãƒˆæˆåŠŸç‡ãŒ{test_summary.get('success_rate', 0)}%ã§ç•°å¸¸ã«ä½ä¸‹ã—ã¦ã„ã¾ã™",
                    metrics
                )
            
        except Exception as e:
            self._log_error(f"ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸºæº–ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
    
    def escalate_to_architect(self, message: str, metrics: Dict[str, Any]) -> None:
        """ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆã¸ã®ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        try:
            escalation_report = {
                "timestamp": datetime.now().isoformat(),
                "escalation_type": "quality_alert",
                "message": message,
                "metrics": metrics,
                "reporter": "tester"
            }
            
            # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜
            escalation_file = self.metrics_path / f"escalation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(escalation_file, 'w', encoding='utf-8') as f:
                json.dump(escalation_report, f, indent=2, ensure_ascii=False)
            
            # tmux_shared_context.mdã¸ã®è¿½è¨˜
            self._update_shared_context(message)
            
            self._log_error(f"ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ: {message}")
            
        except Exception as e:
            self._log_error(f"ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
    
    def _update_shared_context(self, alert_message: str) -> None:
        """å…±æœ‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°"""
        try:
            shared_context_file = self.project_root / "tmux_shared_context.md"
            
            alert_entry = f"\n### ğŸš¨ ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¢ãƒ©ãƒ¼ãƒˆ ({datetime.now().strftime('%a %b %d %H:%M:%S %Z %Y')})\n- {alert_message}\n"
            
            with open(shared_context_file, 'a', encoding='utf-8') as f:
                f.write(alert_entry)
                
        except Exception as e:
            self._log_error(f"å…±æœ‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_metrics(self, metrics: Dict[str, Any]) -> None:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä¿å­˜"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            metrics_file = self.metrics_path / f"quality_metrics_{timestamp}.json"
            
            with open(metrics_file, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)
                
            # æœ€æ–°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æ›´æ–°
            latest_file = self.metrics_path / "latest_quality_metrics.json"
            with open(latest_file, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            self._log_error(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _log_info(self, message: str) -> None:
        """æƒ…å ±ãƒ­ã‚°"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] INFO: {message}\n"
        
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(log_entry)
        except Exception:
            pass  # ãƒ­ã‚°æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
        
        print(log_entry.strip())
    
    def _log_error(self, message: str) -> None:
        """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] ERROR: {message}\n"
        
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(log_entry)
        except Exception:
            pass  # ãƒ­ã‚°æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
        
        print(log_entry.strip())


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒ¼åˆæœŸåŒ–
        monitor = QualityMetricsMonitor()
        
        # è‡ªå‹•å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        metrics = monitor.run_automated_checks()
        
        # çµæœã®è¡¨ç¤º
        print(f"å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº†: {metrics['timestamp']}")
        print(f"ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸: {metrics['quality_metrics']['coverage_report'].get('total', 0)}%")
        print(f"ãƒ†ã‚¹ãƒˆæˆåŠŸç‡: {metrics['quality_metrics']['test_results']['summary'].get('success_rate', 0)}%")
        
        return 0
        
    except Exception as e:
        print(f"å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return 1


if __name__ == "__main__":
    exit(main())