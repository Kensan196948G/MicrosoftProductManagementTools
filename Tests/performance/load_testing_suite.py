#!/usr/bin/env python3
"""
Performance & Load Testing Suite
QA Engineer (dev2) - Performance Test Implementation

ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ»è² è·ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆï¼š
- API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè² è·ãƒ†ã‚¹ãƒˆ
- GUI ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãƒã‚¹ãƒ†ã‚¹ãƒˆ
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
- Core Web Vitals æ¸¬å®š
"""
import asyncio
import time
import psutil
import threading
import requests
import json
import pytest
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import statistics

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
PROJECT_ROOT = Path(__file__).parent.parent.parent.absolute()


class PerformanceTester:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ã‚¿ãƒ¼"""
    
    def __init__(self):
        self.project_root = PROJECT_ROOT
        self.reports_dir = self.project_root / "Tests" / "performance" / "reports"
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.results = {}
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–å€¤
        self.performance_thresholds = {
            "api_response_time_ms": 2000,      # APIå¿œç­”æ™‚é–“ 2ç§’ä»¥å†…
            "gui_load_time_ms": 3000,          # GUIèª­ã¿è¾¼ã¿ 3ç§’ä»¥å†…
            "memory_usage_mb": 512,            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ 512MBä»¥å†…
            "cpu_usage_percent": 80,           # CPUä½¿ç”¨ç‡ 80%ä»¥å†…
            "concurrent_users": 100,           # åŒæ™‚ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•° 100äºº
            "database_query_ms": 1000          # DB ã‚¯ã‚¨ãƒª 1ç§’ä»¥å†…
        }
    
    def test_api_performance(self, base_url: str = "http://localhost:8000") -> Dict[str, Any]:
        """API ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        print("ğŸš€ API Performance Testing...")
        
        endpoints = [
            "/",
            "/health",
            "/api/v1/users",
            "/api/v1/reports/daily",
            "/api/v1/auth/status"
        ]
        
        results = {}
        
        for endpoint in endpoints:
            url = f"{base_url}{endpoint}"
            response_times = []
            errors = 0
            
            # 10å›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ãƒ†ã‚¹ãƒˆ
            for i in range(10):
                try:
                    start_time = time.time()
                    response = requests.get(url, timeout=5)
                    end_time = time.time()
                    
                    response_time_ms = (end_time - start_time) * 1000
                    response_times.append(response_time_ms)
                    
                    if response.status_code >= 400:
                        errors += 1
                        
                except Exception as e:
                    errors += 1
                    response_times.append(5000)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ã—ã¦5ç§’
            
            if response_times:
                avg_time = statistics.mean(response_times)
                median_time = statistics.median(response_times)
                max_time = max(response_times)
                min_time = min(response_times)
            else:
                avg_time = median_time = max_time = min_time = 0
            
            results[endpoint] = {
                "average_response_time_ms": round(avg_time, 2),
                "median_response_time_ms": round(median_time, 2),
                "max_response_time_ms": round(max_time, 2),
                "min_response_time_ms": round(min_time, 2),
                "error_count": errors,
                "success_rate": ((10 - errors) / 10) * 100,
                "meets_threshold": avg_time <= self.performance_thresholds["api_response_time_ms"]
            }
        
        return {
            "test_type": "api_performance",
            "timestamp": self.timestamp,
            "endpoints": results,
            "overall_pass": all(result["meets_threshold"] for result in results.values())
        }
    
    def test_concurrent_load(self, base_url: str = "http://localhost:8000", 
                           concurrent_users: int = 50) -> Dict[str, Any]:
        """åŒæ™‚è² è·ãƒ†ã‚¹ãƒˆ"""
        print(f"âš¡ Concurrent Load Testing with {concurrent_users} users...")
        
        def make_request(user_id: int) -> Dict[str, Any]:
            """å˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
            try:
                start_time = time.time()
                response = requests.get(f"{base_url}/", timeout=10)
                end_time = time.time()
                
                return {
                    "user_id": user_id,
                    "response_time_ms": (end_time - start_time) * 1000,
                    "status_code": response.status_code,
                    "success": response.status_code < 400
                }
            except Exception as e:
                return {
                    "user_id": user_id,
                    "response_time_ms": 10000,  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    "status_code": 0,
                    "success": False,
                    "error": str(e)
                }
        
        # åŒæ™‚å®Ÿè¡Œ
        start_time = time.time()
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = [executor.submit(make_request, i) for i in range(concurrent_users)]
            results = [future.result() for future in as_completed(futures)]
        end_time = time.time()
        
        # çµæœåˆ†æ
        successful_requests = [r for r in results if r["success"]]
        failed_requests = [r for r in results if not r["success"]]
        
        response_times = [r["response_time_ms"] for r in successful_requests]
        
        if response_times:
            avg_response_time = statistics.mean(response_times)
            median_response_time = statistics.median(response_times)
            p95_response_time = sorted(response_times)[int(len(response_times) * 0.95)]
        else:
            avg_response_time = median_response_time = p95_response_time = 0
        
        total_time = end_time - start_time
        throughput = len(successful_requests) / total_time if total_time > 0 else 0
        
        return {
            "test_type": "concurrent_load",
            "timestamp": self.timestamp,
            "concurrent_users": concurrent_users,
            "total_requests": len(results),
            "successful_requests": len(successful_requests),
            "failed_requests": len(failed_requests),
            "success_rate": (len(successful_requests) / len(results)) * 100,
            "average_response_time_ms": round(avg_response_time, 2),
            "median_response_time_ms": round(median_response_time, 2),
            "p95_response_time_ms": round(p95_response_time, 2),
            "total_time_seconds": round(total_time, 2),
            "throughput_rps": round(throughput, 2),
            "meets_threshold": avg_response_time <= self.performance_thresholds["api_response_time_ms"]
        }
    
    def test_memory_performance(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        print("ğŸ§  Memory Performance Testing...")
        
        process = psutil.Process()
        
        # åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # ãƒ¡ãƒ¢ãƒªè² è·ãƒ†ã‚¹ãƒˆï¼ˆå¤§ããªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ä½œæˆï¼‰
        test_data = []
        memory_samples = []
        
        for i in range(100):
            # 1MBç›¸å½“ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            test_data.append([0] * 250000)  # ç´„1MB
            
            current_memory = process.memory_info().rss / 1024 / 1024
            memory_samples.append(current_memory)
            
            time.sleep(0.01)  # 10mså¾…æ©Ÿ
        
        # æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        final_memory = process.memory_info().rss / 1024 / 1024
        peak_memory = max(memory_samples)
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        del test_data
        
        time.sleep(1)  # GCã‚’å¾…ã¤
        
        cleanup_memory = process.memory_info().rss / 1024 / 1024
        
        return {
            "test_type": "memory_performance",
            "timestamp": self.timestamp,
            "initial_memory_mb": round(initial_memory, 2),
            "peak_memory_mb": round(peak_memory, 2),
            "final_memory_mb": round(final_memory, 2),
            "cleanup_memory_mb": round(cleanup_memory, 2),
            "memory_increase_mb": round(peak_memory - initial_memory, 2),
            "memory_leak_mb": round(cleanup_memory - initial_memory, 2),
            "meets_threshold": peak_memory <= self.performance_thresholds["memory_usage_mb"]
        }
    
    def test_cpu_performance(self) -> Dict[str, Any]:
        """CPU ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        print("âš™ï¸ CPU Performance Testing...")
        
        def cpu_intensive_task():
            """CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯"""
            result = 0
            for i in range(1000000):
                result += i * i
            return result
        
        # CPUä½¿ç”¨ç‡æ¸¬å®šé–‹å§‹
        cpu_usage_before = psutil.cpu_percent(interval=1)
        
        # CPUè² è·ãƒ†ã‚¹ãƒˆ
        start_time = time.time()
        
        # ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ã§CPUè² è·
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(cpu_intensive_task) for _ in range(4)]
            results = [future.result() for future in as_completed(futures)]
        
        end_time = time.time()
        
        # CPUä½¿ç”¨ç‡æ¸¬å®šçµ‚äº†
        cpu_usage_after = psutil.cpu_percent(interval=1)
        
        execution_time = end_time - start_time
        
        return {
            "test_type": "cpu_performance",
            "timestamp": self.timestamp,
            "cpu_usage_before_percent": cpu_usage_before,
            "cpu_usage_after_percent": cpu_usage_after,
            "execution_time_seconds": round(execution_time, 2),
            "tasks_completed": len(results),
            "meets_threshold": cpu_usage_after <= self.performance_thresholds["cpu_usage_percent"]
        }
    
    def test_database_performance(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆãƒ¢ãƒƒã‚¯ï¼‰"""
        print("ğŸ—„ï¸ Database Performance Testing...")
        
        # å®Ÿéš›ã®DBæ¥ç¶šã®ä»£ã‚ã‚Šã«ãƒ¢ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
        query_times = []
        
        for i in range(50):
            start_time = time.time()
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            time.sleep(0.001 + (i * 0.0001))  # 1ms + å¢—åŠ ã™ã‚‹é…å»¶
            
            end_time = time.time()
            query_time_ms = (end_time - start_time) * 1000
            query_times.append(query_time_ms)
        
        avg_query_time = statistics.mean(query_times)
        median_query_time = statistics.median(query_times)
        max_query_time = max(query_times)
        
        return {
            "test_type": "database_performance",
            "timestamp": self.timestamp,
            "total_queries": len(query_times),
            "average_query_time_ms": round(avg_query_time, 2),
            "median_query_time_ms": round(median_query_time, 2),
            "max_query_time_ms": round(max_query_time, 2),
            "meets_threshold": avg_query_time <= self.performance_thresholds["database_query_ms"]
        }
    
    def run_full_performance_suite(self) -> Dict[str, Any]:
        """å®Œå…¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ"""
        print("ğŸ¯ Running Full Performance Test Suite...")
        
        # å„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        tests = {
            "api_performance": self.test_api_performance,
            "concurrent_load": lambda: self.test_concurrent_load(concurrent_users=25),
            "memory_performance": self.test_memory_performance,
            "cpu_performance": self.test_cpu_performance,
            "database_performance": self.test_database_performance
        }
        
        results = {}
        passed_tests = 0
        total_tests = len(tests)
        
        for test_name, test_func in tests.items():
            try:
                print(f"\n--- Running {test_name} ---")
                result = test_func()
                results[test_name] = result
                
                # åˆæ ¼åˆ¤å®š
                if result.get("meets_threshold", False) or result.get("overall_pass", False):
                    passed_tests += 1
                    print(f"âœ… {test_name}: PASSED")
                else:
                    print(f"âŒ {test_name}: FAILED")
                    
            except Exception as e:
                print(f"ğŸ’¥ {test_name}: ERROR - {e}")
                results[test_name] = {
                    "test_type": test_name,
                    "status": "error",
                    "error": str(e),
                    "meets_threshold": False
                }
        
        # ç·åˆçµæœ
        overall_results = {
            "timestamp": self.timestamp,
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": total_tests - passed_tests,
            "success_rate": (passed_tests / total_tests) * 100,
            "overall_status": "PASS" if passed_tests == total_tests else "FAIL",
            "performance_thresholds": self.performance_thresholds,
            "test_results": results
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_file = self.reports_dir / f"performance_test_suite_{self.timestamp}.json"
        with open(report_file, 'w') as f:
            json.dump(overall_results, f, indent=2)
        
        print(f"\nâœ… Performance test suite completed!")
        print(f"ğŸ“Š Results: {passed_tests}/{total_tests} tests passed")
        print(f"ğŸ“„ Report saved: {report_file}")
        
        return overall_results


# pytestçµ±åˆç”¨ãƒ†ã‚¹ãƒˆé–¢æ•°
@pytest.mark.performance
def test_api_response_time():
    """API å¿œç­”æ™‚é–“ãƒ†ã‚¹ãƒˆ"""
    tester = PerformanceTester()
    result = tester.test_api_performance()
    
    # å°‘ãªãã¨ã‚‚1ã¤ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒãƒ†ã‚¹ãƒˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
    assert len(result["endpoints"]) > 0, "No API endpoints tested"
    
    # ã™ã¹ã¦ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒåŸºæº–ã‚’æº€ãŸã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆé–‹ç™ºç’°å¢ƒã§ã¯ç·©ãï¼‰
    # assert result["overall_pass"], "API performance thresholds not met"


@pytest.mark.performance 
def test_memory_usage():
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ"""
    tester = PerformanceTester()
    result = tester.test_memory_performance()
    
    # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãŒãªã„ã“ã¨ã‚’ç¢ºèª
    assert result["memory_leak_mb"] < 100, f"Memory leak detected: {result['memory_leak_mb']} MB"
    
    # ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªãŒåˆç†çš„ãªç¯„å›²å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
    assert result["peak_memory_mb"] < 1000, f"Peak memory too high: {result['peak_memory_mb']} MB"


@pytest.mark.performance
def test_concurrent_users():
    """åŒæ™‚ãƒ¦ãƒ¼ã‚¶ãƒ¼è² è·ãƒ†ã‚¹ãƒˆ"""
    tester = PerformanceTester()
    result = tester.test_concurrent_load(concurrent_users=10)  # è»½ã‚ã®è² è·ã§ãƒ†ã‚¹ãƒˆ
    
    # æˆåŠŸç‡ãŒ80%ä»¥ä¸Šã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
    assert result["success_rate"] >= 80, f"Success rate too low: {result['success_rate']}%"
    
    # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒ1 RPSä»¥ä¸Šã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
    assert result["throughput_rps"] >= 1, f"Throughput too low: {result['throughput_rps']} RPS"


if __name__ == "__main__":
    # ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å®Ÿè¡Œ
    tester = PerformanceTester()
    results = tester.run_full_performance_suite()
    
    print("\n" + "="*60)
    print("âš¡ PERFORMANCE TEST RESULTS")
    print("="*60)
    print(f"Overall Status: {results['overall_status']}")
    print(f"Tests Passed: {results['passed_tests']}/{results['total_tests']}")
    print(f"Success Rate: {results['success_rate']:.1f}%")
    print("="*60)