#!/usr/bin/env python3
"""
Context7 APIåˆ¶é™å¯¾å‡¦ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
Microsoft 365 Pythonç§»è¡Œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ - APIåˆ¶é™ç·Šæ€¥å¯¾å‡¦

ğŸš€ ç·Šæ€¥å®Ÿè£…: Context7 APIåˆ¶é™èª²é¡Œå¯¾å‡¦
- ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿
- ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥
- éåŒæœŸå‡¦ç†æœ€é©åŒ–
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–
"""

import asyncio
import aiohttp
import time
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union
from pathlib import Path
import hashlib
from dataclasses import dataclass
from contextlib import asynccontextmanager

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class RateLimitInfo:
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™æƒ…å ±"""
    requests_per_minute: int = 60
    requests_per_hour: int = 1000
    requests_per_day: int = 10000
    current_minute_count: int = 0
    current_hour_count: int = 0
    current_day_count: int = 0
    last_reset_minute: datetime = None
    last_reset_hour: datetime = None
    last_reset_day: datetime = None

class Context7APIOptimizer:
    """
    Context7 APIåˆ¶é™å¯¾å‡¦ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
    ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ãƒ»éåŒæœŸå‡¦ç†æœ€é©åŒ–
    """
    
    def __init__(self, cache_dir: str = "cache/context7"):
        """
        åˆæœŸåŒ–
        
        Args:
            cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†
        self.rate_limit = RateLimitInfo()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        self.cache_ttl_hours = 24  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æœŸé™ï¼ˆæ™‚é–“ï¼‰
        self.memory_cache: Dict[str, Any] = {}
        
        # éåŒæœŸã‚»ãƒƒã‚·ãƒ§ãƒ³
        self.session: Optional[aiohttp.ClientSession] = None
        
        # ãƒãƒƒã‚¯ã‚ªãƒ•è¨­å®š
        self.base_delay = 1.0  # åŸºæœ¬é…å»¶æ™‚é–“ï¼ˆç§’ï¼‰
        self.max_delay = 60.0  # æœ€å¤§é…å»¶æ™‚é–“ï¼ˆç§’ï¼‰
        self.backoff_factor = 2.0  # ãƒãƒƒã‚¯ã‚ªãƒ•å€ç‡
        
        logger.info("âœ… Context7 APIæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼é–‹å§‹"""
        await self.start_session()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼çµ‚äº†"""
        await self.close_session()
    
    async def start_session(self):
        """HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹"""
        if not self.session:
            timeout = aiohttp.ClientTimeout(total=30, connect=10)
            connector = aiohttp.TCPConnector(
                limit=100,  # æœ€å¤§åŒæ™‚æ¥ç¶šæ•°
                limit_per_host=10,  # ãƒ›ã‚¹ãƒˆåˆ¥æœ€å¤§æ¥ç¶šæ•°
                ttl_dns_cache=300,  # DNS ã‚­ãƒ£ãƒƒã‚·ãƒ¥TTL
                use_dns_cache=True
            )
            
            self.session = aiohttp.ClientSession(
                timeout=timeout,
                connector=connector,
                headers={
                    "User-Agent": "Microsoft365Tools-Python/3.0",
                    "Accept": "application/json",
                    "Accept-Encoding": "gzip, deflate"
                }
            )
            logger.info("âœ… HTTP ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹å®Œäº†")
    
    async def close_session(self):
        """HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†"""
        if self.session:
            await self.session.close()
            self.session = None
            logger.info("âœ… HTTP ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†å®Œäº†")
    
    def _generate_cache_key(self, url: str, params: Dict = None) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        cache_data = {"url": url, "params": params or {}}
        cache_str = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_str.encode(), usedforsecurity=False).hexdigest()
    
    def _is_cache_valid(self, cache_file: Path) -> bool:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ€§ç¢ºèª"""
        if not cache_file.exists():
            return False
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ™‚åˆ»ç¢ºèª
        file_mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
        expiry_time = file_mtime + timedelta(hours=self.cache_ttl_hours)
        
        return datetime.now() < expiry_time
    
    async def _load_from_cache(self, cache_key: str) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        try:
            # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
            if cache_key in self.memory_cache:
                logger.debug(f"ğŸ“‹ ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {cache_key[:8]}")
                return self.memory_cache[cache_key]
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
            cache_file = self.cache_dir / f"{cache_key}.json"
            if self._is_cache_valid(cache_file):
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                self.memory_cache[cache_key] = data
                logger.debug(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {cache_key[:8]}")
                return data
            
            return None
            
        except Exception as e:
            logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    async def _save_to_cache(self, cache_key: str, data: Any):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.memory_cache[cache_key] = data
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            cache_file = self.cache_dir / f"{cache_key}.json"
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å®Œäº†: {cache_key[:8]}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _check_rate_limit(self) -> bool:
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯"""
        now = datetime.now()
        
        # åˆ†å˜ä½ãƒªã‚»ãƒƒãƒˆ
        if (not self.rate_limit.last_reset_minute or 
            now.minute != self.rate_limit.last_reset_minute.minute):
            self.rate_limit.current_minute_count = 0
            self.rate_limit.last_reset_minute = now
        
        # æ™‚é–“å˜ä½ãƒªã‚»ãƒƒãƒˆ
        if (not self.rate_limit.last_reset_hour or 
            now.hour != self.rate_limit.last_reset_hour.hour):
            self.rate_limit.current_hour_count = 0
            self.rate_limit.last_reset_hour = now
        
        # æ—¥å˜ä½ãƒªã‚»ãƒƒãƒˆ
        if (not self.rate_limit.last_reset_day or 
            now.date() != self.rate_limit.last_reset_day.date()):
            self.rate_limit.current_day_count = 0
            self.rate_limit.last_reset_day = now
        
        # åˆ¶é™ãƒã‚§ãƒƒã‚¯
        if self.rate_limit.current_minute_count >= self.rate_limit.requests_per_minute:
            logger.warning("âš ï¸ åˆ†å˜ä½ãƒ¬ãƒ¼ãƒˆåˆ¶é™åˆ°é”")
            return False
        
        if self.rate_limit.current_hour_count >= self.rate_limit.requests_per_hour:
            logger.warning("âš ï¸ æ™‚é–“å˜ä½ãƒ¬ãƒ¼ãƒˆåˆ¶é™åˆ°é”")
            return False
        
        if self.rate_limit.current_day_count >= self.rate_limit.requests_per_day:
            logger.warning("âš ï¸ æ—¥å˜ä½ãƒ¬ãƒ¼ãƒˆåˆ¶é™åˆ°é”")
            return False
        
        return True
    
    def _increment_rate_limit(self):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼å¢—åŠ """
        self.rate_limit.current_minute_count += 1
        self.rate_limit.current_hour_count += 1
        self.rate_limit.current_day_count += 1
    
    async def _wait_for_rate_limit(self):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™è§£é™¤å¾…æ©Ÿ"""
        if not self._check_rate_limit():
            # æ¬¡ã®åˆ†ã¾ã§å¾…æ©Ÿ
            now = datetime.now()
            next_minute = now.replace(second=0, microsecond=0) + timedelta(minutes=1)
            wait_seconds = (next_minute - now).total_seconds()
            
            logger.info(f"â³ ãƒ¬ãƒ¼ãƒˆåˆ¶é™è§£é™¤å¾…æ©Ÿ: {wait_seconds:.1f}ç§’")
            await asyncio.sleep(wait_seconds)
    
    async def _make_request_with_retry(self, url: str, params: Dict = None, max_retries: int = 3) -> Optional[Dict]:
        """ãƒªãƒˆãƒ©ã‚¤ä»˜ãHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
        delay = self.base_delay
        
        for attempt in range(max_retries + 1):
            try:
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
                await self._wait_for_rate_limit()
                
                # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
                async with self.session.get(url, params=params) as response:
                    self._increment_rate_limit()
                    
                    if response.status == 200:
                        data = await response.json()
                        logger.info(f"âœ… API ãƒªã‚¯ã‚¨ã‚¹ãƒˆæˆåŠŸ: {url}")
                        return data
                    
                    elif response.status == 429:  # Too Many Requests
                        retry_after = int(response.headers.get('Retry-After', delay))
                        logger.warning(f"âš ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ (429): {retry_after}ç§’å¾…æ©Ÿ")
                        await asyncio.sleep(retry_after)
                        continue
                    
                    elif response.status >= 500:  # Server Error
                        logger.warning(f"âš ï¸ ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ ({response.status}): ãƒªãƒˆãƒ©ã‚¤ {attempt + 1}/{max_retries}")
                        if attempt < max_retries:
                            await asyncio.sleep(delay)
                            delay = min(delay * self.backoff_factor, self.max_delay)
                            continue
                    
                    else:
                        logger.error(f"âŒ API ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ ({response.status}): {url}")
                        return None
            
            except asyncio.TimeoutError:
                logger.warning(f"âš ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼: ãƒªãƒˆãƒ©ã‚¤ {attempt + 1}/{max_retries}")
                if attempt < max_retries:
                    await asyncio.sleep(delay)
                    delay = min(delay * self.backoff_factor, self.max_delay)
                    continue
            
            except Exception as e:
                logger.error(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                if attempt < max_retries:
                    await asyncio.sleep(delay)
                    delay = min(delay * self.backoff_factor, self.max_delay)
                    continue
                break
        
        logger.error(f"âŒ æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°åˆ°é”: {url}")
        return None
    
    async def fetch_with_optimization(self, url: str, params: Dict = None, force_refresh: bool = False) -> Optional[Dict]:
        """
        æœ€é©åŒ–ã•ã‚ŒãŸAPIå–å¾—
        
        Args:
            url: å–å¾—URL
            params: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            force_refresh: å¼·åˆ¶ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥
            
        Returns:
            å–å¾—ãƒ‡ãƒ¼ã‚¿
        """
        cache_key = self._generate_cache_key(url, params)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèªï¼ˆå¼·åˆ¶ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã§ãªã„å ´åˆï¼‰
        if not force_refresh:
            cached_data = await self._load_from_cache(cache_key)
            if cached_data:
                return cached_data
        
        # API ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
        data = await self._make_request_with_retry(url, params)
        
        if data:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            await self._save_to_cache(cache_key, data)
        
        return data
    
    async def batch_fetch(self, requests: List[Dict], max_concurrent: int = 5) -> List[Optional[Dict]]:
        """
        ãƒãƒƒãƒå–å¾—ï¼ˆä¸¦è¡Œå‡¦ç†ï¼‰
        
        Args:
            requests: ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒªã‚¹ãƒˆ [{"url": str, "params": dict}, ...]
            max_concurrent: æœ€å¤§ä¸¦è¡Œæ•°
            
        Returns:
            å–å¾—çµæœãƒªã‚¹ãƒˆ
        """
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def fetch_with_semaphore(request_data: Dict) -> Optional[Dict]:
            async with semaphore:
                return await self.fetch_with_optimization(
                    request_data["url"],
                    request_data.get("params")
                )
        
        # ä¸¦è¡Œå®Ÿè¡Œ
        tasks = [fetch_with_semaphore(req) for req in requests]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ä¾‹å¤–ã‚’ None ã«å¤‰æ›
        processed_results = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"âŒ ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {result}")
                processed_results.append(None)
            else:
                processed_results.append(result)
        
        success_count = sum(1 for r in processed_results if r is not None)
        logger.info(f"ğŸ“Š ãƒãƒƒãƒå‡¦ç†å®Œäº†: {success_count}/{len(requests)} æˆåŠŸ")
        
        return processed_results
    
    def get_rate_limit_status(self) -> Dict[str, Any]:
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™çŠ¶æ³å–å¾—"""
        return {
            "requests_per_minute": {
                "limit": self.rate_limit.requests_per_minute,
                "current": self.rate_limit.current_minute_count,
                "remaining": self.rate_limit.requests_per_minute - self.rate_limit.current_minute_count
            },
            "requests_per_hour": {
                "limit": self.rate_limit.requests_per_hour,
                "current": self.rate_limit.current_hour_count,
                "remaining": self.rate_limit.requests_per_hour - self.rate_limit.current_hour_count
            },
            "requests_per_day": {
                "limit": self.rate_limit.requests_per_day,
                "current": self.rate_limit.current_day_count,
                "remaining": self.rate_limit.requests_per_day - self.rate_limit.current_day_count
            },
            "last_resets": {
                "minute": self.rate_limit.last_reset_minute.isoformat() if self.rate_limit.last_reset_minute else None,
                "hour": self.rate_limit.last_reset_hour.isoformat() if self.rate_limit.last_reset_hour else None,
                "day": self.rate_limit.last_reset_day.isoformat() if self.rate_limit.last_reset_day else None
            }
        }
    
    def clear_cache(self, pattern: Optional[str] = None):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        try:
            # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
            if pattern:
                keys_to_remove = [k for k in self.memory_cache.keys() if pattern in k]
                for key in keys_to_remove:
                    del self.memory_cache[key]
            else:
                self.memory_cache.clear()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
            cache_files = list(self.cache_dir.glob("*.json"))
            if pattern:
                cache_files = [f for f in cache_files if pattern in f.name]
            
            for cache_file in cache_files:
                cache_file.unlink()
            
            logger.info(f"ğŸ—‘ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å®Œäº†: {len(cache_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
            
        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}")

# Context7å°‚ç”¨æœ€é©åŒ–ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
class Context7OptimizedClient:
    """Context7å°‚ç”¨æœ€é©åŒ–ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        self.optimizer = Context7APIOptimizer()
        self.base_url = "https://api.context7.com"  # ä»®ã®URL
    
    async def resolve_library_id(self, library_name: str) -> Optional[Dict]:
        """ãƒ©ã‚¤ãƒ–ãƒ©ãƒªIDè§£æ±ºï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        url = f"{self.base_url}/resolve"
        params = {"library_name": library_name}
        
        async with self.optimizer:
            return await self.optimizer.fetch_with_optimization(url, params)
    
    async def get_library_docs(self, library_id: str, topic: str = None, tokens: int = 8000) -> Optional[Dict]:
        """ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        url = f"{self.base_url}/docs"
        params = {
            "library_id": library_id,
            "topic": topic,
            "tokens": tokens
        }
        
        async with self.optimizer:
            return await self.optimizer.fetch_with_optimization(url, params)
    
    async def batch_resolve_libraries(self, library_names: List[str]) -> List[Optional[Dict]]:
        """è¤‡æ•°ãƒ©ã‚¤ãƒ–ãƒ©ãƒªIDä¸€æ‹¬è§£æ±º"""
        requests = [
            {"url": f"{self.base_url}/resolve", "params": {"library_name": name}}
            for name in library_names
        ]
        
        async with self.optimizer:
            return await self.optimizer.batch_fetch(requests, max_concurrent=3)

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
async def test_context7_optimization():
    """Context7æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª Context7 APIæœ€é©åŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    client = Context7OptimizedClient()
    
    # å˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ†ã‚¹ãƒˆ
    result = await client.resolve_library_id("PyQt6")
    print(f"ğŸ“‹ å˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆçµæœ: {result is not None}")
    
    # ãƒãƒƒãƒãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ†ã‚¹ãƒˆ
    libraries = ["PyQt6", "FastAPI", "Microsoft Graph SDK"]
    batch_results = await client.batch_resolve_libraries(libraries)
    success_count = sum(1 for r in batch_results if r is not None)
    print(f"ğŸ“Š ãƒãƒƒãƒãƒªã‚¯ã‚¨ã‚¹ãƒˆçµæœ: {success_count}/{len(libraries)}")
    
    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™çŠ¶æ³ç¢ºèª
    async with client.optimizer:
        status = client.optimizer.get_rate_limit_status()
        print(f"ğŸ“ˆ ãƒ¬ãƒ¼ãƒˆåˆ¶é™çŠ¶æ³: {status}")

if __name__ == "__main__":
    asyncio.run(test_context7_optimization())